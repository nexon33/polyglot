//! Round 14 Pentest Attack Tests for poly-verified.
//!
//! Findings:
//! V14-01 HIGH:    Verified<T> map() chain accumulates unbounded proof detachment — N maps produce
//!                 a value with zero relationship to the original proof's output_hash
//! V14-02 HIGH:    Disclosure partial-reveal output_binding not verified — only all-revealed checks
//!                 tokens_hash, partial can have arbitrary output_binding
//! V14-03 HIGH:    HashIvc verify accepts proof with all-identical checkpoints — no transition
//!                 uniqueness enforced, allows single-step replay
//! V14-04 HIGH:    CompositeProof ordering sensitivity — same proofs in different order produce
//!                 different composition_hash, but both verify. Attacker can reorder inner proofs
//! V14-05 HIGH:    VerifiedResponse from_bytes allows proof_len=0 — empty proof bytes pass parsing
//!                 but validate_proof_bytes fails, creating a split between parsers
//! V14-06 HIGH:    Disclosure splicing with execution_proof from one disclosure and tokens/proofs
//!                 from another — partial redaction bypass
//! V14-07 HIGH:    HashIvc proof with duplicate consecutive checkpoints — identical state transitions
//!                 accepted, allowing padding attacks on step_count
//! V14-08 HIGH:    VerifiedResponse code_hash field not validated against proof content in
//!                 validate_header_consistency — only scheme and privacy checked
//! V14-09 MEDIUM:  FixedPoint precision loss accumulation — repeated multiply-divide cycles lose bits
//! V14-10 MEDIUM:  FixedPoint division asymmetry — a/b * b != a due to truncation, breaks
//!                 verified arithmetic reversibility
//! V14-11 MEDIUM:  Merkle tree with all-duplicate leaves — all proofs have identical siblings,
//!                 allowing cross-position proof reuse
//! V14-12 MEDIUM:  CompositeProof with self-referential proof (outer == inner) — accepted
//! V14-13 MEDIUM:  CodeAttestation sign_message does not include a timestamp — unbounded replay window
//! V14-14 MEDIUM:  HashIvc proof checkpoints are raw transition hashes — leaks computation internals
//!                 even in Transparent mode (state_before, step_inputs, state_after recoverable)
//! V14-15 MEDIUM:  Disclosure verify accepts Mock execution_proof with forged output_hash that
//!                 happens to match crafted tokens
//! V14-16 MEDIUM:  VerifiedResponse verifier_key_hash not validated — any 32 bytes accepted
//! V14-17 MEDIUM:  Commitment constant-time eq but total_checkpoints XOR leaks on timing for zero
//! V14-18 MEDIUM:  MerkleProof serialization: sibling_count uses big-endian but leaf_index uses
//!                 little-endian — mixed endianness confusion
//! V14-19 MEDIUM:  FixedPoint exp_approx with negative input returns wrong sign for even terms
//! V14-20 MEDIUM:  HashIvc accumulator clone allows proof forking — clone acc, finalize both
//! V14-21 LOW:     Verified<Vec<u32>> with empty vec — disclosure on empty token set
//! V14-22 LOW:     FixedPoint Mul with MAX and ONE should be identity but saturates
//! V14-23 LOW:     CompositeProof serde preserves composition_hash but doesn't re-derive it
//! V14-24 LOW:     MerkleTree::build with 2 leaves — minimal tree edge case
//! V14-25 LOW:     HashIvc proof with Private mode and ZERO blinding_commitment
//! V14-26 LOW:     Disclosure reveal same token at adjacent positions — token collision
//! V14-27 LOW:     VerifiedResponse wire format version — no version field for future compat
//! V14-28 LOW:     FixedPoint Neg of MIN saturates — not MIN itself
//! V14-29 LOW:     Commitment from_bytes rejects off-by-one sizes
//! V14-30 LOW:     SignedCommitment re-signing with different key changes public_key field
//! V14-31 LOW:     ProofNode from_bytes with invalid is_left byte values > 0x01
//! V14-32 LOW:     HashIvc finalize with single step has chain tip == hash_chain_step(ZERO, transition)
//! V14-33 LOW:     Disclosure with large token set — 1000 tokens performance
//! V14-34 LOW:     FixedPoint from_decimal saturates on overflow correctly
//! V14-35 LOW:     MerkleTree with 5 leaves (odd, not power of two) — proof integrity at all indices
//! V14-36 LOW:     Verified<T> Display leaks inner value — no redaction for private mode
//! V14-37 LOW:     CompositeProof compose with many distinct privacy modes — escalation path
//! V14-38 LOW:     VerifiedResponse from_bytes rejects truncated proof section
//! V14-39 LOW:     HashIvc proof verify with swapped input/output hashes
//! V14-40 LOW:     Disclosure revealed token Merkle proof leaf_index cross-check
//! V14-41 LOW:     FixedPoint comparison edge case: values straddling zero
//! V14-42 LOW:     CodeAttestation sign_message deterministic for same inputs

use sha2::{Digest, Sha256};

use poly_verified::crypto::hash::{
    hash_chain_step, hash_combine, hash_data, hash_transition,
};
use poly_verified::crypto::merkle::{verify_proof, MerkleTree};
use poly_verified::crypto::signing::{sign_attestation, verify_attestation};
use poly_verified::crypto::commitment::{
    create_commitment, sign_commitment, verify_signed_commitment,
};
use poly_verified::disclosure::{
    create_disclosure, disclosure_output_hash,
    verify_disclosure, Disclosure, DisclosedToken,
};
use poly_verified::fixed_point::FixedPoint;
use poly_verified::ivc::hash_ivc::HashIvc;
use poly_verified::ivc::IvcBackend;
use poly_verified::proof_composition::CompositeProof;
use poly_verified::proof_serialize::VerifiedResponse;
use poly_verified::types::{
    CodeAttestation, Commitment, Hash, PrivacyMode, ProofNode,
    StepWitness, VerifiedProof, ZERO_HASH,
};
use poly_verified::verified_type::Verified;

use ed25519_dalek::SigningKey;

// ========================================================================
// Helpers
// ========================================================================

fn tokens_hash(tokens: &[u32]) -> Hash {
    let mut hasher = Sha256::new();
    for &t in tokens {
        hasher.update(t.to_le_bytes());
    }
    hasher.finalize().into()
}

fn valid_hash_ivc_proof_for_tokens(tokens: &[u32]) -> VerifiedProof {
    let ivc = HashIvc;
    let code_hash = [0x03; 32];
    let mut acc = ivc.init(&code_hash, PrivacyMode::Transparent);
    let witness = StepWitness {
        state_before: hash_data(b"before"),
        state_after: hash_data(b"after"),
        step_inputs: hash_data(b"inputs"),
    };
    ivc.fold_step(&mut acc, &witness).unwrap();
    acc.input_hash = ZERO_HASH;
    // [V14-02 FIX] Use combined output hash that binds token Merkle root
    acc.output_hash = disclosure_output_hash(tokens);
    ivc.finalize(acc).unwrap()
}

fn make_verified(tokens: Vec<u32>) -> Verified<Vec<u32>> {
    let proof = valid_hash_ivc_proof_for_tokens(&tokens);
    Verified::__macro_new(tokens, proof)
}

// ========================================================================
// V14-01: Verified<T> map() chain accumulates unbounded proof detachment
// ========================================================================

/// ATTACK: map() preserves the proof but transforms the value. After N
/// sequential maps, the value has been transformed N times while the proof
/// remains from the original computation. The proof's output_hash binds to
/// the ORIGINAL value, not any of the mapped intermediates or final result.
/// An attacker can use map() to produce any desired value while keeping a
/// "valid" proof from a completely different computation.
#[test]
fn v14_01_map_chain_proof_detachment() {
    let tokens = vec![10, 20, 30, 40];
    let verified = make_verified(tokens.clone());

    // Chain of maps: tokens -> sum -> string -> length -> completely different
    let original_proof_output = match verified.proof() {
        VerifiedProof::HashIvc { output_hash, .. } => *output_hash,
        _ => panic!("expected HashIvc"),
    };

    let step1 = verified.map(|v| v.iter().sum::<u32>()); // 100
    let step2 = step1.map(|s| format!("result={}", s));   // "result=100"
    let step3 = step2.map(|s| s.len());                   // 10
    let final_val = step3.map(|_| 999999u64);              // completely unrelated

    // The proof still claims to verify
    assert!(final_val.is_verified());

    // But the proof's output_hash is still from the original tokens
    let final_proof_output = match final_val.proof() {
        VerifiedProof::HashIvc { output_hash, .. } => *output_hash,
        _ => panic!("expected HashIvc"),
    };
    assert_eq!(
        original_proof_output, final_proof_output,
        "V14-01: After 4 maps, proof output_hash unchanged — detached from actual value"
    );

    // The actual value 999999 has NOTHING to do with the proof
    assert_eq!(*final_val.value(), 999999u64);
}

// ========================================================================
// V14-02: Disclosure partial-reveal output_binding not verified
// ========================================================================

/// ATTACK: When a disclosure has some redacted tokens, verify_disclosure
/// does NOT recompute tokens_hash (because redacted token values are unknown).
/// This means the output_binding field is only cross-checked against the
/// execution_proof's output_hash, NOT against the actual tokens. An attacker
/// can craft a disclosure with correct Merkle proofs for revealed tokens but
/// a fabricated output_binding that matches a different execution proof.
#[test]
fn v14_02_partial_reveal_output_binding_unchecked() {
    let tokens_a = vec![100, 200, 300, 400];
    let tokens_b = vec![100, 200, 999, 888]; // Same first two, different rest

    let verified_a = make_verified(tokens_a.clone());
    let verified_b = make_verified(tokens_b.clone());

    // Create disclosure from A revealing positions 0,1 (these match in both sets)
    let disclosure_a = create_disclosure(&verified_a, &[0, 1]).unwrap();

    // Create disclosure from B (just to get its execution_proof and output_binding)
    let disclosure_b = create_disclosure(&verified_b, &[0, 1]).unwrap();

    // Splice: take A's tokens/proofs/output_root but B's execution_proof and output_binding
    let spliced = Disclosure {
        tokens: disclosure_a.tokens.clone(),
        proofs: disclosure_a.proofs.clone(),
        output_root: disclosure_a.output_root,
        total_tokens: disclosure_a.total_tokens,
        execution_proof: disclosure_b.execution_proof.clone(),
        output_binding: disclosure_b.output_binding,
    };

    // [V14-02 FIX] After fix: verify_disclosure computes
    // hash_combine(output_binding, output_root) and checks it against
    // execution_proof.output_hash. The splice has:
    // - output_binding = tokens_hash(B) [raw]
    // - output_root = A's Merkle root
    // - execution_proof.output_hash = hash_combine(tokens_hash(B), B's root)
    // So: hash_combine(tokens_hash(B), A_root) != hash_combine(tokens_hash(B), B_root)
    // The splice is caught because A_root != B_root.
    assert!(
        !verify_disclosure(&spliced),
        "V14-02 FIX: Spliced disclosure caught — output_root bound to execution_proof"
    );
}

/// [V14-02 FIX] Verify that all-redacted splice is now caught after fix.
/// Before the fix, an attacker could take redacted leaf hashes from
/// disclosure A and execution_proof from disclosure B, and the splice
/// would pass verification because output_root was not bound into
/// output_binding. After the fix, output_binding = hash_combine(tokens_hash,
/// token_merkle_root), so A's output_root and B's output_binding are
/// incompatible.
#[test]
fn v14_02_all_redacted_splice_caught() {
    let tokens_a = vec![100, 200, 300, 400];
    let tokens_b = vec![500, 600, 700, 800];

    let verified_a = make_verified(tokens_a);
    let verified_b = make_verified(tokens_b);

    let disclosure_a = create_disclosure(&verified_a, &[]).unwrap();
    let disclosure_b = create_disclosure(&verified_b, &[]).unwrap();

    // Splice: A's tokens/output_root with B's execution_proof and output_binding
    let spliced = Disclosure {
        tokens: disclosure_a.tokens.clone(),
        proofs: disclosure_a.proofs.clone(),
        output_root: disclosure_a.output_root,
        total_tokens: disclosure_a.total_tokens,
        execution_proof: disclosure_b.execution_proof.clone(),
        output_binding: disclosure_b.output_binding,
    };

    // [V14-02 FIX] After fix: output_binding includes the token Merkle root.
    // B's output_binding = hash_combine(tokens_hash(B), B_merkle_root).
    // B's execution_proof.output_hash = hash_combine(tokens_hash(B), B_merkle_root).
    // output_binding == execution_proof.output_hash passes.
    // But Merkle reconstruction from A's leaf hashes produces A's output_root,
    // which matches disclosure.output_root (A's root). This passes.
    // HOWEVER, the all_revealed check reconstructs output_binding from the tokens
    // — but this is all-redacted, so that check is skipped.
    // The key defense: output_binding (B's) matches execution_proof (B's).
    // But the Merkle tree from A's leaves produces A's root, not B's root.
    // Since output_binding already includes B's root, and the disclosure has A's root,
    // the binding is inconsistent. verify_disclosure catches this because
    // output_binding == output_hash passes, BUT the reconstructed Merkle root
    // (A's root) doesn't match the root encoded in output_binding (B's root).
    // The exact mechanism: output_binding was computed WITH B's merkle root,
    // so it's a different value than what A's root would produce.
    assert!(
        !verify_disclosure(&spliced),
        "V14-02 FIX: All-redacted splice now correctly rejected — \
         output_binding includes token Merkle root"
    );
}

// ========================================================================
// V14-03: HashIvc accepts all-identical checkpoints (replay attack)
// ========================================================================

/// ATTACK: Create a HashIvc proof where all checkpoints are the same hash.
/// This means the "computation" performed the exact same state transition
/// at every step. The system accepts this, allowing step_count inflation
/// by replaying a single step N times.
#[test]
fn v14_03_identical_checkpoints_accepted() {
    let ivc = HashIvc;
    let code_hash = hash_data(b"func");
    let mut acc = ivc.init(&code_hash, PrivacyMode::Transparent);

    // Same witness for every step — identical checkpoints
    let w = StepWitness {
        state_before: hash_data(b"state"),
        state_after: hash_data(b"state"),
        step_inputs: hash_data(b"input"),
    };

    for _ in 0..5 {
        ivc.fold_step(&mut acc, &w).unwrap();
    }

    acc.input_hash = hash_data(b"in");
    acc.output_hash = hash_data(b"out");
    let proof = ivc.finalize(acc).unwrap();

    // All 5 checkpoints are the same transition hash
    match &proof {
        VerifiedProof::HashIvc { checkpoints, step_count, .. } => {
            assert_eq!(*step_count, 5);
            // Verify all checkpoints are identical
            for i in 1..checkpoints.len() {
                assert_eq!(
                    checkpoints[0], checkpoints[i],
                    "V14-03: All checkpoints are identical"
                );
            }
        }
        _ => panic!("expected HashIvc"),
    }

    // This proof verifies because hash chain and Merkle tree don't reject
    // duplicate entries — the chain is deterministic from the sequence.
    assert!(
        ivc.verify(&proof, &hash_data(b"in"), &hash_data(b"out")).unwrap(),
        "V14-03: Proof with all-identical checkpoints accepted — \
         allows step count inflation via single-step replay"
    );
}

// ========================================================================
// V14-04: CompositeProof ordering sensitivity
// ========================================================================

/// ATTACK: The composition_hash depends on the ORDER of inner proofs.
/// Same set of proofs in different order produces different composition_hash.
/// Both orders verify independently. An attacker can reorder inner proofs
/// to produce a different composite that appears valid but represents a
/// different computation order than actually occurred.
#[test]
fn v14_04_composite_ordering_sensitivity() {
    let outer = VerifiedProof::Mock {
        input_hash: ZERO_HASH,
        output_hash: ZERO_HASH,
        privacy_mode: PrivacyMode::Transparent,
    };
    let inner_a = VerifiedProof::Mock {
        input_hash: [0x01; 32],
        output_hash: [0x02; 32],
        privacy_mode: PrivacyMode::Transparent,
    };
    let inner_b = VerifiedProof::Mock {
        input_hash: [0x03; 32],
        output_hash: [0x04; 32],
        privacy_mode: PrivacyMode::Transparent,
    };

    // Order 1: A then B
    let comp_ab = CompositeProof::compose(
        outer.clone(),
        vec![inner_a.clone(), inner_b.clone()],
    );
    // Order 2: B then A
    let comp_ba = CompositeProof::compose(
        outer.clone(),
        vec![inner_b.clone(), inner_a.clone()],
    );

    // Both verify
    assert!(comp_ab.verify_composition());
    assert!(comp_ba.verify_composition());

    // But they have different composition hashes — different "identities"
    assert_ne!(
        comp_ab.composition_hash, comp_ba.composition_hash,
        "V14-04: Different inner proof ordering produces different composition_hash"
    );

    // This means an attacker can present inner proofs in any order and
    // each ordering appears to be a distinct, valid composite.
    // Whether this is a vulnerability depends on whether ordering matters.
    // For call-sequence verification, ordering IS significant, so both
    // composites should be valid but represent different computation paths.
}

// ========================================================================
// V14-05: VerifiedResponse with proof_len=0
// ========================================================================

/// ATTACK: Craft a VerifiedResponse with proof_len=0 (empty proof_bytes).
/// from_bytes accepts this, but validate_proof_bytes fails on empty JSON.
/// This creates a gap between "parseable" and "valid".
#[test]
fn v14_05_empty_proof_bytes() {
    let proof = VerifiedProof::Mock {
        input_hash: ZERO_HASH,
        output_hash: ZERO_HASH,
        privacy_mode: PrivacyMode::Transparent,
    };
    let response = VerifiedResponse::new(&proof, ZERO_HASH, b"data".to_vec(), ZERO_HASH);
    let original_bytes = response.to_bytes();

    // Find proof_len offset (98) and craft with proof_len=0
    let mut crafted = Vec::new();
    crafted.extend_from_slice(&original_bytes[..98]); // header through privacy_mode
    crafted.extend_from_slice(&0u32.to_be_bytes());   // proof_len = 0
    // No proof bytes
    crafted.extend_from_slice(&ZERO_HASH);             // verifier_key_hash
    crafted.extend_from_slice(b"data");                 // value_bytes

    let decoded = VerifiedResponse::from_bytes(&crafted).unwrap();
    assert_eq!(decoded.proof_bytes.len(), 0);

    // validate_proof_bytes should fail for empty proof_bytes
    assert!(
        !decoded.validate_proof_bytes(),
        "V14-05: Empty proof_bytes must fail validation"
    );

    // But the response was successfully parsed — potential for receivers
    // that don't call validate_proof_bytes to accept garbage
}

// ========================================================================
// V14-06: Disclosure execution_proof splice attack
// ========================================================================

/// ATTACK: Take a fully-revealed disclosure from computation A, and replace
/// its execution_proof with one from computation B (which has the same token
/// values but different execution path). If both happen to produce the same
/// output_hash, the splice goes undetected.
#[test]
fn v14_06_execution_proof_splice_same_tokens() {
    let tokens = vec![100, 200, 300, 400];

    // Two different computations that produce the same output tokens
    let ivc = HashIvc;
    let code_hash_a = hash_data(b"function_a");
    let code_hash_b = hash_data(b"function_b");

    let combined_output = disclosure_output_hash(&tokens);

    let mut acc_a = ivc.init(&code_hash_a, PrivacyMode::Transparent);
    let w_a = StepWitness {
        state_before: hash_data(b"a_before"),
        state_after: hash_data(b"a_after"),
        step_inputs: hash_data(b"a_input"),
    };
    ivc.fold_step(&mut acc_a, &w_a).unwrap();
    acc_a.input_hash = ZERO_HASH;
    acc_a.output_hash = combined_output;
    let proof_a = ivc.finalize(acc_a).unwrap();

    let mut acc_b = ivc.init(&code_hash_b, PrivacyMode::Transparent);
    let w_b = StepWitness {
        state_before: hash_data(b"b_before"),
        state_after: hash_data(b"b_after"),
        step_inputs: hash_data(b"b_input"),
    };
    ivc.fold_step(&mut acc_b, &w_b).unwrap();
    acc_b.input_hash = ZERO_HASH;
    acc_b.output_hash = combined_output;
    let proof_b = ivc.finalize(acc_b).unwrap();

    // Build disclosure with A's proof
    let verified_a = Verified::__macro_new(tokens.clone(), proof_a);
    let disclosure_a = create_disclosure(&verified_a, &[0, 1, 2, 3]).unwrap();
    assert!(verify_disclosure(&disclosure_a));

    // Splice: swap in B's execution_proof (same output_hash, different code)
    let spliced = Disclosure {
        tokens: disclosure_a.tokens.clone(),
        proofs: disclosure_a.proofs.clone(),
        output_root: disclosure_a.output_root,
        total_tokens: disclosure_a.total_tokens,
        execution_proof: proof_b.clone(),
        output_binding: disclosure_a.output_binding,
    };

    // This should verify because:
    // 1. Merkle proofs are valid for the same tokens (same leaves)
    // 2. output_binding == tokens_hash(tokens) matches both proof_a and proof_b
    // 3. proof_b has the same output_hash as proof_a
    // The only difference is the code_hash inside the proof.
    assert!(
        verify_disclosure(&spliced),
        "V14-06: Execution proof splice with same output_hash is undetectable — \
         code provenance is lost. An attacker can substitute any proof that \
         produces the same output_hash."
    );
}

// ========================================================================
// V14-07: HashIvc with duplicate consecutive checkpoints
// ========================================================================

/// ATTACK: A computation that transitions to the same state repeatedly
/// produces duplicate consecutive checkpoints. The system accepts this,
/// allowing an attacker to pad step_count arbitrarily by repeating
/// identity transitions.
#[test]
fn v14_07_duplicate_consecutive_checkpoints() {
    let ivc = HashIvc;
    let code_hash = hash_data(b"func");
    let mut acc = ivc.init(&code_hash, PrivacyMode::Transparent);

    // First step: real computation
    let w_real = StepWitness {
        state_before: hash_data(b"init"),
        state_after: hash_data(b"result"),
        step_inputs: hash_data(b"real_input"),
    };
    ivc.fold_step(&mut acc, &w_real).unwrap();

    // Pad with 99 identity transitions
    let w_identity = StepWitness {
        state_before: hash_data(b"result"),
        state_after: hash_data(b"result"),
        step_inputs: hash_data(b"noop"),
    };
    for _ in 0..99 {
        ivc.fold_step(&mut acc, &w_identity).unwrap();
    }

    acc.input_hash = hash_data(b"in");
    acc.output_hash = hash_data(b"out");
    let proof = ivc.finalize(acc).unwrap();

    // Proof has 100 steps but only 1 real computation
    match &proof {
        VerifiedProof::HashIvc { step_count, checkpoints, .. } => {
            assert_eq!(*step_count, 100);
            // Check the padding steps produce the same transition hash
            for i in 2..checkpoints.len() {
                assert_eq!(
                    checkpoints[1], checkpoints[i],
                    "V14-07: Identity transitions produce duplicate checkpoints"
                );
            }
        }
        _ => panic!("expected HashIvc"),
    }

    assert!(
        ivc.verify(&proof, &hash_data(b"in"), &hash_data(b"out")).unwrap(),
        "V14-07: Padded proof with 99 identity steps accepted — step_count inflation"
    );
}

// ========================================================================
// V14-08: VerifiedResponse code_hash not in validate_header_consistency
// ========================================================================

/// ATTACK: validate_header_consistency checks proof_scheme and privacy_mode
/// against the proof_bytes content, but does NOT check code_hash. An attacker
/// can tamper with the wire header's code_hash and it won't be detected by
/// the consistency check.
#[test]
fn v14_08_response_code_hash_not_in_consistency_check() {
    let ivc = HashIvc;
    let code_hash = hash_data(b"real_code");
    let mut acc = ivc.init(&code_hash, PrivacyMode::Transparent);
    let w = StepWitness {
        state_before: hash_data(b"b"),
        state_after: hash_data(b"a"),
        step_inputs: hash_data(b"i"),
    };
    ivc.fold_step(&mut acc, &w).unwrap();
    let proof = ivc.finalize(acc).unwrap();

    let response = VerifiedResponse::new(&proof, ZERO_HASH, b"val".to_vec(), ZERO_HASH);
    let mut bytes = response.to_bytes();

    // Tamper code_hash in wire header (bytes 64..96)
    bytes[64..96].copy_from_slice(&[0xFF; 32]);

    let decoded = VerifiedResponse::from_bytes(&bytes).unwrap();

    // validate_header_consistency only checks scheme + privacy — passes!
    assert!(
        decoded.validate_header_consistency(),
        "V14-08: Tampered code_hash not detected by validate_header_consistency"
    );

    // The code_hash in the header is now 0xFF*32, but the proof's code_hash
    // is the real one. A receiver trusting the header is misled.
    assert_eq!(decoded.code_hash, [0xFF; 32]);
}

// ========================================================================
// V14-09: FixedPoint precision loss accumulation
// ========================================================================

/// ATTACK: Repeated multiply-divide cycles accumulate precision loss.
/// In a verified computation, this can cause the final result to drift
/// significantly from the mathematically correct answer.
#[test]
fn v14_09_fixedpoint_precision_loss_accumulation() {
    // Use a factor that doesn't divide evenly into the fixed-point representation
    // to guarantee truncation loss per cycle.
    let mut val = FixedPoint::from_int(1);
    let factor = FixedPoint::from_int(3);

    // Multiply by 3 then divide by 3 loses fractional part each cycle.
    // 1 * 3 = 3, 3 / 3 = 1 exactly. Need asymmetric ops to force loss.
    // Instead: divide then multiply (1/3 truncates, then *3 doesn't recover).
    for _ in 0..100 {
        val = val / factor;
        val = val * factor;
    }

    // After 100 divide-then-multiply cycles, precision loss accumulates
    let original_raw = FixedPoint::from_int(1).raw();
    let raw_drift = (val.raw() - original_raw).unsigned_abs();

    // The drift should be nonzero because 1/3 in fixed-point truncates
    assert!(
        raw_drift > 0,
        "V14-09: Precision loss from divide-multiply cycles (drift={} ticks)",
        raw_drift
    );

    // Despite raw drift, the integer part may still show 0 (since it started at 1
    // and only lost sub-unit precision)
    assert!(
        val.raw() <= original_raw,
        "V14-09: Divide-multiply cycle can only lose precision, never gain"
    );
}

// ========================================================================
// V14-10: FixedPoint division asymmetry — a/b * b != a
// ========================================================================

/// In exact arithmetic, (a / b) * b == a. But FixedPoint truncates during
/// division, so the roundtrip loses information. This can cause verified
/// computations to produce different results depending on operation order.
#[test]
fn v14_10_fixedpoint_division_asymmetry() {
    let a = FixedPoint::from_int(7);
    let b = FixedPoint::from_int(3);

    let ab_then_mul = (a / b) * b; // (7/3)*3 should be 7
    let ba_then_div = a; // just a

    // (7/3) truncates the fractional part, so (7/3)*3 != 7
    assert_ne!(
        ab_then_mul.raw(), ba_then_div.raw(),
        "V14-10: (a/b)*b != a due to truncation — non-reversible division"
    );

    // The integer parts may look the same, but raw values differ
    assert_eq!(ab_then_mul.to_i64(), 6, "V14-10: (7/3)*3 truncates to 6");
    assert_eq!(ba_then_div.to_i64(), 7, "V14-10: Original is 7");
}

// ========================================================================
// V14-11: Merkle tree with all-duplicate leaves
// ========================================================================

/// When all leaves are identical, all proofs have identical sibling hashes
/// at every level. This means a proof for position 0 can be reused for
/// position 1 (just by changing leaf_index), because the path hashes
/// are the same regardless of position.
#[test]
fn v14_11_merkle_duplicate_leaves_proof_reuse() {
    let same_leaf = hash_data(b"same");
    let leaves = vec![same_leaf; 4];
    let tree = MerkleTree::build(&leaves);

    let proof_0 = tree.generate_proof(0, &ZERO_HASH).unwrap();
    let proof_1 = tree.generate_proof(1, &ZERO_HASH).unwrap();

    // Both proofs verify
    assert!(verify_proof(&proof_0));
    assert!(verify_proof(&proof_1));

    // The root is the same (obviously)
    assert_eq!(proof_0.root, proof_1.root);

    // The leaf hashes are the same
    assert_eq!(proof_0.leaf, proof_1.leaf);

    // In a disclosure context, if all tokens have the same value,
    // the Merkle proof for any position can be used for any other position.
    // The only defense is the leaf_index check in verify_disclosure.
}

// ========================================================================
// V14-12: CompositeProof self-referential (outer == inner)
// ========================================================================

/// ATTACK: Create a CompositeProof where the outer proof is also in the
/// inner proofs list. This creates a circular reference where the same
/// proof is both the "caller" and "callee."
#[test]
fn v14_12_composite_self_referential() {
    let proof = VerifiedProof::Mock {
        input_hash: hash_data(b"x"),
        output_hash: hash_data(b"y"),
        privacy_mode: PrivacyMode::Transparent,
    };

    // Self-referential: proof is both outer and inner
    let composite = CompositeProof::compose(proof.clone(), vec![proof.clone()]);

    // This verifies — no check for self-reference
    assert!(
        composite.verify_composition(),
        "V14-12: Self-referential composite (outer == inner) accepted — \
         no circular dependency detection"
    );

    // With multiple copies
    let composite_3 = CompositeProof::compose(
        proof.clone(),
        vec![proof.clone(), proof.clone(), proof.clone()],
    );
    assert!(composite_3.verify_composition());
    assert_eq!(composite_3.proof_count(), 4);
}

// ========================================================================
// V14-13: CodeAttestation has no timestamp — unbounded replay
// ========================================================================

/// ATTACK: A CodeAttestation signed today can be replayed indefinitely
/// because sign_message() only includes node_id, code_hash, circuit_id.
/// There is no timestamp, nonce, or expiry.
#[test]
fn v14_13_attestation_no_timestamp() {
    let key = SigningKey::from_bytes(&[0x42; 32]);
    let node_id = [0xAA; 32];
    let code_hash = hash_data(b"code_v1");

    let attestation = sign_attestation(&node_id, &code_hash, 1, &key);

    // Verify it works
    assert!(verify_attestation(&attestation, &key.verifying_key()).is_ok());

    // The signed message is just node_id || code_hash || circuit_id
    let msg = attestation.sign_message();
    assert_eq!(msg.len(), 72);
    // No timestamp in the message
    assert_eq!(&msg[0..32], &node_id);
    assert_eq!(&msg[32..64], &code_hash);
    assert_eq!(&msg[64..72], &1u64.to_le_bytes());

    // Same attestation verifies forever — no expiry mechanism
    // An attacker who captures this attestation can replay it indefinitely
    // even after the code has been updated to a new version.
    let attestation_copy = attestation.clone();
    assert!(
        verify_attestation(&attestation_copy, &key.verifying_key()).is_ok(),
        "V14-13: Attestation is replayable — no timestamp or nonce in signed message"
    );
}

// ========================================================================
// V14-14: HashIvc checkpoints leak computation internals
// ========================================================================

/// The checkpoints in a HashIvc proof are transition hashes:
/// hash_transition(state_before, step_inputs, state_after).
/// While the preimage can't be recovered from the hash, having the
/// checkpoint hashes in the proof allows an observer to verify whether
/// a specific (state_before, step_inputs, state_after) tuple was part
/// of the computation by computing hash_transition and comparing.
#[test]
fn v14_14_checkpoints_leak_via_transition_hash() {
    let ivc = HashIvc;
    let code_hash = hash_data(b"func");
    let mut acc = ivc.init(&code_hash, PrivacyMode::Transparent);

    let secret_before = hash_data(b"secret_state_before");
    let secret_input = hash_data(b"secret_step_input");
    let secret_after = hash_data(b"secret_state_after");

    let w = StepWitness {
        state_before: secret_before,
        state_after: secret_after,
        step_inputs: secret_input,
    };
    ivc.fold_step(&mut acc, &w).unwrap();
    let proof = ivc.finalize(acc).unwrap();

    // An observer who suspects specific (before, input, after) can verify
    let suspected_transition = hash_transition(&secret_before, &secret_input, &secret_after);

    match &proof {
        VerifiedProof::HashIvc { checkpoints, .. } => {
            // The checkpoint IS the transition hash
            assert_eq!(
                checkpoints[0], suspected_transition,
                "V14-14: Checkpoint reveals transition hash — observer can verify \
                 suspected state transitions against the proof"
            );
        }
        _ => panic!("expected HashIvc"),
    }
}

// ========================================================================
// V14-15: Disclosure with Mock proof and crafted output_hash
// ========================================================================

/// ATTACK: Create a Mock proof with output_hash = tokens_hash(desired_tokens).
/// Then build a disclosure with that proof. verify_disclosure accepts Mock
/// proofs with matching output_hash.
#[test]
fn v14_15_mock_proof_crafted_output() {
    let tokens = vec![42, 43, 44, 45];
    let crafted_proof = VerifiedProof::Mock {
        input_hash: ZERO_HASH,
        output_hash: disclosure_output_hash(&tokens),
        privacy_mode: PrivacyMode::Transparent,
    };

    let verified = Verified::__macro_new(tokens.clone(), crafted_proof);
    let disclosure = create_disclosure(&verified, &[0, 1, 2, 3]).unwrap();

    // Mock proof with matching output_hash passes disclosure verification
    assert!(
        verify_disclosure(&disclosure),
        "V14-15: Mock proof with crafted output_hash accepted in disclosure — \
         anyone can forge disclosures in test/mock mode"
    );
}

// ========================================================================
// V14-16: VerifiedResponse verifier_key_hash not validated
// ========================================================================

/// The verifier_key_hash field in VerifiedResponse is not checked or
/// validated anywhere. It's just stored and returned. An attacker can
/// set it to any value without affecting verification.
#[test]
fn v14_16_verifier_key_hash_unvalidated() {
    let proof = VerifiedProof::Mock {
        input_hash: ZERO_HASH,
        output_hash: ZERO_HASH,
        privacy_mode: PrivacyMode::Transparent,
    };

    let response = VerifiedResponse::new(&proof, ZERO_HASH, b"data".to_vec(), [0xFF; 32]);
    let bytes = response.to_bytes();
    let decoded = VerifiedResponse::from_bytes(&bytes).unwrap();

    // verifier_key_hash is whatever we set — not validated
    assert_eq!(decoded.verifier_key_hash, [0xFF; 32]);

    // value_integrity still passes regardless of verifier_key_hash
    assert!(decoded.verify_value_integrity());
    assert!(decoded.validate_header_consistency());

    // Change verifier_key_hash in wire format
    let mut tampered = bytes.clone();
    let proof_len = u32::from_be_bytes(tampered[98..102].try_into().unwrap()) as usize;
    let vk_offset = 102 + proof_len;
    tampered[vk_offset..vk_offset + 32].copy_from_slice(&[0x00; 32]);

    let decoded2 = VerifiedResponse::from_bytes(&tampered).unwrap();
    assert_eq!(decoded2.verifier_key_hash, [0x00; 32]);
    assert!(
        decoded2.verify_value_integrity(),
        "V14-16: Tampered verifier_key_hash has no effect on validation"
    );
}

// ========================================================================
// V14-17: Commitment PartialEq timing for total_checkpoints
// ========================================================================

/// Commitment PartialEq uses XOR for total_checkpoints (constant-time).
/// Verify this works correctly for edge cases.
#[test]
fn v14_17_commitment_eq_checkpoint_edge_cases() {
    let base = Commitment {
        root: [0x11; 32],
        total_checkpoints: 0,
        chain_tip: [0x22; 32],
        code_hash: [0x33; 32],
    };

    let zero_checkpoints = base.clone();
    assert_eq!(base, zero_checkpoints);

    // Different checkpoint counts
    let mut one = base.clone();
    one.total_checkpoints = 1;
    assert_ne!(base, one);

    // u64::MAX
    let mut max = base.clone();
    max.total_checkpoints = u64::MAX;
    assert_ne!(base, max);

    // Same large values
    let mut large_a = base.clone();
    large_a.total_checkpoints = u64::MAX;
    let mut large_b = base.clone();
    large_b.total_checkpoints = u64::MAX;
    assert_eq!(large_a, large_b);

    // Adjacent values
    let mut adj_a = base.clone();
    adj_a.total_checkpoints = 999999;
    let mut adj_b = base.clone();
    adj_b.total_checkpoints = 1000000;
    assert_ne!(adj_a, adj_b);
}

// ========================================================================
// V14-18: MerkleProof mixed endianness in serialization
// ========================================================================

/// MerkleProof uses big-endian for sibling_count (u32) but little-endian
/// for leaf_index (u64). This mixed endianness is a bug-prone design
/// that could cause cross-platform issues.
#[test]
fn v14_18_merkle_proof_mixed_endianness() {
    let leaves = vec![hash_data(b"a"), hash_data(b"b"), hash_data(b"c"), hash_data(b"d")];
    let tree = MerkleTree::build(&leaves);
    let proof = tree.generate_proof(2, &ZERO_HASH).unwrap();

    let bytes = proof.to_bytes();

    // leaf_index at bytes[32..40] is little-endian
    let leaf_index_le = u64::from_le_bytes(bytes[32..40].try_into().unwrap());
    assert_eq!(leaf_index_le, 2, "leaf_index is little-endian");

    // sibling_count at bytes[40..44] is big-endian
    let sibling_count_be = u32::from_be_bytes(bytes[40..44].try_into().unwrap());
    assert_eq!(sibling_count_be, 2, "sibling_count is big-endian");

    // Mixed endianness confirmed — document the inconsistency
    // A parser that uses wrong endianness for either field will get garbage
    let sibling_count_wrong = u32::from_le_bytes(bytes[40..44].try_into().unwrap());
    assert_ne!(
        sibling_count_wrong, 2,
        "V14-18: Using wrong endianness for sibling_count produces wrong value"
    );
}

// ========================================================================
// V14-19: FixedPoint exp_approx with negative input
// ========================================================================

/// exp(-x) should be a small positive number for large x.
/// The Taylor series may diverge or produce incorrect signs for
/// intermediate terms, potentially giving wrong results.
#[test]
fn v14_19_fixedpoint_exp_negative() {
    // e^(-1) ≈ 0.3679
    let neg_one = FixedPoint::from_int(-1);
    let result = neg_one.exp_approx(20);

    // Should be positive and close to 0.3679
    assert!(result.raw() > 0, "V14-19: e^(-1) should be positive");
    let expected_min = FixedPoint::from_decimal(36, 2).raw(); // 0.36
    let expected_max = FixedPoint::from_decimal(37, 2).raw(); // 0.37
    assert!(
        result.raw() >= expected_min && result.raw() <= expected_max,
        "V14-19: e^(-1) ≈ 0.3679, got raw={}",
        result.raw()
    );

    // e^(-5) ≈ 0.0067 — very small
    let neg_five = FixedPoint::from_int(-5);
    let result5 = neg_five.exp_approx(30);
    assert!(
        result5.raw() > 0,
        "V14-19: e^(-5) should be positive, got raw={}",
        result5.raw()
    );
}

// ========================================================================
// V14-20: HashIvc accumulator clone allows proof forking
// ========================================================================

/// ATTACK: The HashIvcAccumulator is Clone. An attacker can clone the
/// accumulator mid-computation, diverge the two copies, and finalize both.
/// This produces two valid proofs that share a common prefix but diverge —
/// a proof fork.
#[test]
fn v14_20_accumulator_clone_fork() {
    let ivc = HashIvc;
    let code_hash = hash_data(b"func");
    let mut acc = ivc.init(&code_hash, PrivacyMode::Transparent);

    // Shared prefix
    let w_shared = StepWitness {
        state_before: hash_data(b"init"),
        state_after: hash_data(b"shared"),
        step_inputs: hash_data(b"common"),
    };
    ivc.fold_step(&mut acc, &w_shared).unwrap();

    // Fork: clone and diverge
    let mut acc_fork = acc.clone();

    // Branch A: continue with different step
    let w_a = StepWitness {
        state_before: hash_data(b"shared"),
        state_after: hash_data(b"result_a"),
        step_inputs: hash_data(b"input_a"),
    };
    ivc.fold_step(&mut acc, &w_a).unwrap();
    acc.input_hash = hash_data(b"in");
    acc.output_hash = hash_data(b"out_a");
    let proof_a = ivc.finalize(acc).unwrap();

    // Branch B: different divergence
    let w_b = StepWitness {
        state_before: hash_data(b"shared"),
        state_after: hash_data(b"result_b"),
        step_inputs: hash_data(b"input_b"),
    };
    ivc.fold_step(&mut acc_fork, &w_b).unwrap();
    acc_fork.input_hash = hash_data(b"in");
    acc_fork.output_hash = hash_data(b"out_b");
    let proof_b = ivc.finalize(acc_fork).unwrap();

    // Both proofs verify independently
    assert!(ivc.verify(&proof_a, &hash_data(b"in"), &hash_data(b"out_a")).unwrap());
    assert!(ivc.verify(&proof_b, &hash_data(b"in"), &hash_data(b"out_b")).unwrap());

    // But they represent incompatible computation histories
    match (&proof_a, &proof_b) {
        (
            VerifiedProof::HashIvc { chain_tip: tip_a, checkpoints: cp_a, .. },
            VerifiedProof::HashIvc { chain_tip: tip_b, checkpoints: cp_b, .. },
        ) => {
            // First checkpoint is shared
            assert_eq!(cp_a[0], cp_b[0], "V14-20: Shared prefix checkpoint matches");
            // Second checkpoint diverges
            assert_ne!(cp_a[1], cp_b[1], "V14-20: Fork produces different checkpoints");
            // Different chain tips
            assert_ne!(tip_a, tip_b, "V14-20: Forked chains have different tips");
        }
        _ => panic!("expected HashIvc"),
    }
}

// ========================================================================
// V14-21: Disclosure on empty token vec
// ========================================================================

/// Edge case: Verified<Vec<u32>> with empty vec. Disclosure creation should
/// handle this gracefully.
#[test]
fn v14_21_disclosure_empty_tokens() {
    let empty_tokens: Vec<u32> = vec![];
    let proof = VerifiedProof::Mock {
        input_hash: ZERO_HASH,
        output_hash: tokens_hash(&empty_tokens),
        privacy_mode: PrivacyMode::Transparent,
    };
    let verified = Verified::__macro_new(empty_tokens, proof);

    // Creating a disclosure with no indices should work
    let disclosure = create_disclosure(&verified, &[]).unwrap();
    assert_eq!(disclosure.total_tokens, 0);

    // But verification should reject it (0 tokens is invalid)
    assert!(
        !verify_disclosure(&disclosure),
        "V14-21: Disclosure with 0 tokens must be rejected by verify_disclosure"
    );
}

// ========================================================================
// V14-22: FixedPoint Mul MAX * ONE saturates
// ========================================================================

/// Multiplying by ONE should be an identity operation, but for values near
/// i128::MAX, the intermediate product overflows and saturates.
#[test]
fn v14_22_fixedpoint_mul_max_one_saturates() {
    let big = FixedPoint::from_raw(i128::MAX);
    let one = FixedPoint::ONE;

    let result = big * one;

    // Ideally big * 1 == big, but the intermediate (big.raw * one.raw)
    // overflows i128 because one.raw = 2^48 and big.raw = i128::MAX.
    // checked_mul returns None, so we saturate.
    assert_eq!(
        result.raw(),
        i128::MAX,
        "V14-22: MAX * ONE saturates to MAX (not identity)"
    );

    // For values small enough that raw * SCALE doesn't overflow i128,
    // the identity works. i128::MAX / SCALE^2 is the threshold.
    // SCALE = 2^48, so raw < i128::MAX / 2^48 ≈ 2^79
    // from_int(1000) has raw = 1000 * 2^48, and 1000*2^48 * 2^48 = 1000*2^96 < 2^127
    let medium = FixedPoint::from_int(1000);
    let result2 = medium * one;
    assert_eq!(
        result2.raw(),
        medium.raw(),
        "V14-22: Medium * ONE is identity (no overflow)"
    );
}

// ========================================================================
// V14-23: CompositeProof serde doesn't re-derive composition_hash
// ========================================================================

/// ATTACK: Deserialize a CompositeProof, change an inner proof's content
/// (but not the composition_hash). verify_composition should catch this
/// because it recomputes the hash from the proof contents.
#[test]
fn v14_23_composite_serde_inner_tamper() {
    let outer = VerifiedProof::Mock {
        input_hash: ZERO_HASH,
        output_hash: hash_data(b"out"),
        privacy_mode: PrivacyMode::Transparent,
    };
    let inner = VerifiedProof::Mock {
        input_hash: hash_data(b"inner_in"),
        output_hash: hash_data(b"inner_out"),
        privacy_mode: PrivacyMode::Transparent,
    };

    let composite = CompositeProof::compose(outer, vec![inner]);
    assert!(composite.verify_composition());

    // Serialize, tamper inner proof, deserialize
    let mut json = serde_json::to_value(&composite).unwrap();
    if let Some(inners) = json.get_mut("inner_proofs") {
        if let Some(arr) = inners.as_array_mut() {
            if let Some(first) = arr.first_mut() {
                if let Some(mock) = first.get_mut("Mock") {
                    mock["output_hash"] = serde_json::json!(vec![0xFFu8; 32]);
                }
            }
        }
    }
    let tampered: CompositeProof = serde_json::from_value(json).unwrap();

    // verify_composition recomputes hash — detects the tamper
    assert!(
        !tampered.verify_composition(),
        "V14-23: Tampered inner proof detected by composition_hash recomputation"
    );
}

// ========================================================================
// V14-24: MerkleTree with 2 leaves — minimal tree
// ========================================================================

/// Edge case: 2-leaf tree. Both proofs should verify.
#[test]
fn v14_24_merkle_two_leaves() {
    let leaves = vec![hash_data(b"left"), hash_data(b"right")];
    let tree = MerkleTree::build(&leaves);

    assert_eq!(tree.layers.len(), 2); // leaf layer + root layer
    assert_eq!(tree.root, hash_combine(&leaves[0], &leaves[1]));

    for i in 0..2u64 {
        let proof = tree.generate_proof(i, &ZERO_HASH).unwrap();
        assert_eq!(proof.siblings.len(), 1);
        assert!(verify_proof(&proof), "Proof for index {} must verify", i);
    }

    // Proof for index 0: sibling is leaf 1, is_left=false
    let p0 = tree.generate_proof(0, &ZERO_HASH).unwrap();
    assert_eq!(p0.siblings[0].hash, leaves[1]);
    assert!(!p0.siblings[0].is_left);

    // Proof for index 1: sibling is leaf 0, is_left=true
    let p1 = tree.generate_proof(1, &ZERO_HASH).unwrap();
    assert_eq!(p1.siblings[0].hash, leaves[0]);
    assert!(p1.siblings[0].is_left);
}

// ========================================================================
// V14-25: HashIvc Private mode with ZERO blinding_commitment
// ========================================================================

/// ATTACK: Craft a HashIvc proof with Private mode but set
/// blinding_commitment to ZERO_HASH via serde. The verify should
/// reject because the recomputed blinding won't be ZERO_HASH.
#[test]
fn v14_25_private_zero_blinding() {
    let ivc = HashIvc;
    let code_hash = hash_data(b"func");
    let mut acc = ivc.init(&code_hash, PrivacyMode::Private);
    let w = StepWitness {
        state_before: hash_data(b"b"),
        state_after: hash_data(b"a"),
        step_inputs: hash_data(b"i"),
    };
    ivc.fold_step(&mut acc, &w).unwrap();
    let proof = ivc.finalize(acc).unwrap();

    // Tamper: set blinding_commitment to ZERO
    let mut json = serde_json::to_value(&proof).unwrap();
    if let Some(hashivc) = json.get_mut("HashIvc") {
        hashivc["blinding_commitment"] = serde_json::json!(ZERO_HASH);
    }
    let tampered: VerifiedProof = serde_json::from_value(json).unwrap();

    // The real blinding_commitment is derived from checkpoints + counters
    // and is non-zero. Setting it to zero should fail verification.
    assert!(
        !ivc.verify(&tampered, &ZERO_HASH, &ZERO_HASH).unwrap(),
        "V14-25: Zero blinding_commitment in Private mode must be rejected"
    );
}

// ========================================================================
// V14-26: Disclosure with repeated token values
// ========================================================================

/// When the same token value appears at multiple positions, the leaf
/// hashes are identical. Verify that the system correctly handles this.
#[test]
fn v14_26_disclosure_repeated_token_values() {
    let tokens = vec![42, 42, 42, 42]; // All same
    let verified = make_verified(tokens);

    // Reveal positions 0 and 2
    let disclosure = create_disclosure(&verified, &[0, 2]).unwrap();

    // Both revealed tokens have the same token_id
    match (&disclosure.tokens[0], &disclosure.tokens[2]) {
        (
            DisclosedToken::Revealed { token_id: id0, .. },
            DisclosedToken::Revealed { token_id: id2, .. },
        ) => {
            assert_eq!(id0, id2, "Same token values at different positions");
        }
        _ => panic!("expected Revealed"),
    }

    // Both proofs should have the same leaf hash
    assert_eq!(disclosure.proofs[0].leaf, disclosure.proofs[1].leaf);

    // But different leaf_index values
    assert_ne!(disclosure.proofs[0].leaf_index, disclosure.proofs[1].leaf_index);

    assert!(
        verify_disclosure(&disclosure),
        "V14-26: Disclosure with repeated token values must verify"
    );
}

// ========================================================================
// V14-27: VerifiedResponse no version field
// ========================================================================

/// The wire format has no version number. Any future changes to the format
/// would break backward compatibility with no way to detect old vs new format.
#[test]
fn v14_27_response_no_version() {
    let proof = VerifiedProof::Mock {
        input_hash: ZERO_HASH,
        output_hash: ZERO_HASH,
        privacy_mode: PrivacyMode::Transparent,
    };
    let response = VerifiedResponse::new(&proof, ZERO_HASH, b"data".to_vec(), ZERO_HASH);
    let bytes = response.to_bytes();

    // First byte is value_hash[0] — no magic number or version prefix
    // Any future format change has no way to distinguish from current format
    assert_eq!(
        bytes.len(),
        32 + 32 + 32 + 1 + 1 + 4 + response.proof_bytes.len() + 32 + 4,
        "V14-27: Wire format has fixed structure, no version field"
    );

    // Verify the first 32 bytes are value_hash (not a version prefix)
    let value_hash = hash_data(b"data");
    assert_eq!(&bytes[0..32], &value_hash);
}

// ========================================================================
// V14-28: FixedPoint Neg of MIN saturates
// ========================================================================

/// Negating i128::MIN should produce i128::MAX (saturating), not panic.
#[test]
fn v14_28_fixedpoint_neg_min() {
    let min = FixedPoint::from_raw(i128::MIN);
    let negated = -min;

    // Saturating negation: -(MIN) = MAX (not MIN itself)
    assert_eq!(
        negated.raw(),
        i128::MAX,
        "V14-28: Negation of MIN saturates to MAX"
    );

    // Negating MAX gives -MAX (which is MIN+1, not MIN)
    let max = FixedPoint::from_raw(i128::MAX);
    let neg_max = -max;
    assert_eq!(neg_max.raw(), -i128::MAX);
}

// ========================================================================
// V14-29: Commitment from_bytes rejects off-by-one sizes
// ========================================================================

/// Verify that from_bytes rejects sizes that are exactly 1 byte too
/// short or too long.
#[test]
fn v14_29_commitment_off_by_one() {
    let commitment = Commitment {
        root: [0x11; 32],
        total_checkpoints: 42,
        chain_tip: [0x22; 32],
        code_hash: [0x33; 32],
    };
    let bytes = commitment.to_bytes();

    // One byte short
    assert!(
        Commitment::from_bytes(&bytes[..103]).is_err(),
        "V14-29: 103 bytes (too short by 1) must be rejected"
    );

    // One byte extra
    let mut extra = bytes.to_vec();
    extra.push(0x00);
    assert!(
        Commitment::from_bytes(&extra).is_err(),
        "V14-29: 105 bytes (too long by 1) must be rejected"
    );

    // Exact size works
    assert!(Commitment::from_bytes(&bytes).is_ok());
}

// ========================================================================
// V14-30: SignedCommitment re-signing with different key
// ========================================================================

/// Sign a commitment with key A, then create a new SignedCommitment
/// with key B's signature. The public_key changes, and the old signature
/// no longer matches.
#[test]
fn v14_30_signed_commitment_key_change() {
    let checkpoints: Vec<Hash> = (0..4u8).map(|i| hash_data(&[i])).collect();
    let code_hash = hash_data(b"code");
    let (commitment, _) = create_commitment(&checkpoints, &code_hash);

    let key_a = SigningKey::from_bytes(&[0x42; 32]);
    let key_b = SigningKey::from_bytes(&[0x43; 32]);

    let signed_a = sign_commitment(&commitment, &key_a);
    let signed_b = sign_commitment(&commitment, &key_b);

    // Both verify with their respective keys
    assert!(verify_signed_commitment(&signed_a).is_ok());
    assert!(verify_signed_commitment(&signed_b).is_ok());

    // Different signatures
    assert_ne!(signed_a.signature, signed_b.signature);

    // Different public keys
    assert_ne!(signed_a.public_key, signed_b.public_key);

    // Cross-verify should fail (signed_a with key_b's public_key)
    let mut cross = signed_a.clone();
    cross.public_key = signed_b.public_key;
    assert!(
        verify_signed_commitment(&cross).is_err(),
        "V14-30: Signature verification fails with wrong public key"
    );
}

// ========================================================================
// V14-31: ProofNode from_bytes with invalid is_left values
// ========================================================================

/// ProofNode's is_left byte should only be 0x00 or 0x01. Values > 0x01
/// must be rejected.
#[test]
fn v14_31_proof_node_invalid_is_left() {
    let valid = ProofNode {
        hash: [0xAA; 32],
        is_left: true,
    };
    let bytes = valid.to_bytes();
    assert!(ProofNode::from_bytes(&bytes).is_ok());

    // Try all invalid is_left values
    for invalid_byte in 2..=255u8 {
        let mut bad = bytes;
        bad[32] = invalid_byte;
        assert!(
            ProofNode::from_bytes(&bad).is_err(),
            "V14-31: is_left=0x{:02x} must be rejected",
            invalid_byte
        );
    }
}

// ========================================================================
// V14-32: HashIvc single-step chain tip structure
// ========================================================================

/// Verify the exact structure of chain_tip for a single-step proof.
/// chain_tip = hash_combine(hash_combine(hash_chain_step(ZERO, transition), code_binding), mode_binding)
#[test]
fn v14_32_single_step_chain_tip() {
    let ivc = HashIvc;
    let code_hash = hash_data(b"func");
    let mut acc = ivc.init(&code_hash, PrivacyMode::Transparent);

    let w = StepWitness {
        state_before: hash_data(b"before"),
        state_after: hash_data(b"after"),
        step_inputs: hash_data(b"inputs"),
    };
    ivc.fold_step(&mut acc, &w).unwrap();
    acc.input_hash = hash_data(b"in");
    acc.output_hash = hash_data(b"out");
    let proof = ivc.finalize(acc).unwrap();

    // Manually compute expected chain_tip
    let transition = hash_transition(
        &hash_data(b"before"),
        &hash_data(b"inputs"),
        &hash_data(b"after"),
    );
    let raw_tip = hash_chain_step(&ZERO_HASH, &transition);
    let code_binding = hash_data(&code_hash);
    let mode_binding = hash_data(&[PrivacyMode::Transparent as u8]);
    let expected_tip = hash_combine(
        &hash_combine(&raw_tip, &code_binding),
        &mode_binding,
    );

    match &proof {
        VerifiedProof::HashIvc { chain_tip, .. } => {
            assert_eq!(
                *chain_tip, expected_tip,
                "V14-32: Single-step chain_tip matches manual computation"
            );
        }
        _ => panic!("expected HashIvc"),
    }
}

// ========================================================================
// V14-33: Disclosure with large token set (performance)
// ========================================================================

/// Verify disclosure works correctly with 1000 tokens.
#[test]
fn v14_33_large_disclosure() {
    let tokens: Vec<u32> = (0..1000).collect();
    let verified = make_verified(tokens);

    // Reveal every 10th token
    let indices: Vec<usize> = (0..1000).step_by(10).collect();
    let disclosure = create_disclosure(&verified, &indices).unwrap();

    assert_eq!(disclosure.total_tokens, 1000);
    assert_eq!(disclosure.proofs.len(), 100);
    assert!(
        verify_disclosure(&disclosure),
        "V14-33: Disclosure with 1000 tokens must verify"
    );
}

// ========================================================================
// V14-34: FixedPoint from_decimal saturation on overflow
// ========================================================================

/// from_decimal with large value that overflows value * SCALE should saturate.
#[test]
fn v14_34_fixedpoint_from_decimal_overflow() {
    // i64::MAX * SCALE doesn't overflow i128 (i64::MAX is ~2^63, SCALE is 2^48,
    // product is ~2^111 < 2^127). So from_decimal(i64::MAX, 0) works normally.
    let result = FixedPoint::from_decimal(i64::MAX, 0);
    let expected = (i64::MAX as i128) * FixedPoint::SCALE;
    assert_eq!(
        result.raw(),
        expected,
        "V14-34: from_decimal with i64::MAX does NOT overflow (fits in i128)"
    );

    // To actually overflow: from_int also uses checked_mul... but from_int
    // doesn't use checked_mul, it uses plain multiplication. So very large
    // values via from_int would wrap. from_decimal does use checked_mul.
    // We need value * SCALE to overflow i128. Since SCALE = 2^48, we need
    // value > i128::MAX / 2^48 ≈ 2^79. i64::MAX ≈ 2^63, which is fine.
    // But i64 can't hold 2^79. So we can't trigger the overflow branch
    // via from_decimal(i64, u32) with reasonable inputs.

    // Large decimal_places >= 38 returns ZERO
    let result_big_dp = FixedPoint::from_decimal(1000, 38);
    assert_eq!(
        result_big_dp.raw(),
        0,
        "V14-34: from_decimal with 38 decimal places returns ZERO"
    );

    // decimal_places = 37: largest valid, divisor = 10^37
    let result_37 = FixedPoint::from_decimal(1000, 37);
    // 1000 * SCALE / 10^37 = very tiny but nonzero
    assert_eq!(
        result_37.to_i64(),
        0,
        "V14-34: from_decimal with 37 decimal places produces sub-unit value"
    );
}

// ========================================================================
// V14-35: MerkleTree with 5 leaves (odd, not power-of-two)
// ========================================================================

/// 5 leaves: leaf layer has odd count, requiring duplication. Verify all
/// proofs are valid.
#[test]
fn v14_35_merkle_five_leaves() {
    let leaves: Vec<Hash> = (0..5u8).map(|i| hash_data(&[i])).collect();
    let tree = MerkleTree::build(&leaves);

    // Layer 0: 5 leaves
    // Layer 1: ceil(5/2) = 3 (leaf 4 is duplicated with itself)
    // Layer 2: ceil(3/2) = 2 (node 2 is duplicated with itself)
    // Layer 3: 1 (root)
    assert_eq!(tree.layers.len(), 4);
    assert_eq!(tree.layers[0].len(), 5);
    assert_eq!(tree.layers[1].len(), 3);
    assert_eq!(tree.layers[2].len(), 2);
    assert_eq!(tree.layers[3].len(), 1);

    for i in 0..5u64 {
        let proof = tree.generate_proof(i, &ZERO_HASH).unwrap();
        assert!(
            verify_proof(&proof),
            "V14-35: Proof for index {} in 5-leaf tree must verify",
            i
        );
    }
}

// ========================================================================
// V14-36: Verified<T> Display leaks value even in Private mode
// ========================================================================

/// Verified<T> implements Display as "Verified(value)". For Private mode
/// proofs, this still prints the value, potentially leaking information.
#[test]
fn v14_36_verified_display_leaks_value() {
    let proof = VerifiedProof::Mock {
        input_hash: ZERO_HASH,
        output_hash: ZERO_HASH,
        privacy_mode: PrivacyMode::Private,
    };
    let verified = Verified::__macro_new(42u64, proof);

    assert!(verified.is_private());

    let display = format!("{}", verified);
    assert!(
        display.contains("42"),
        "V14-36: Verified<T> Display leaks value '42' even in Private mode: '{}'",
        display
    );
}

// ========================================================================
// V14-37: CompositeProof privacy escalation
// ========================================================================

/// Test that mixing all three privacy modes correctly escalates to Private.
#[test]
fn v14_37_composite_privacy_escalation() {
    let transparent = VerifiedProof::Mock {
        input_hash: ZERO_HASH,
        output_hash: ZERO_HASH,
        privacy_mode: PrivacyMode::Transparent,
    };
    let private_inputs = VerifiedProof::Mock {
        input_hash: ZERO_HASH,
        output_hash: ZERO_HASH,
        privacy_mode: PrivacyMode::PrivateInputs,
    };
    let private = VerifiedProof::Mock {
        input_hash: ZERO_HASH,
        output_hash: ZERO_HASH,
        privacy_mode: PrivacyMode::Private,
    };

    // Transparent + PrivateInputs = PrivateInputs
    let comp1 = CompositeProof::compose(
        transparent.clone(),
        vec![private_inputs.clone()],
    );
    assert_eq!(comp1.privacy_mode, PrivacyMode::PrivateInputs);
    assert!(comp1.verify_composition());

    // Transparent + Private = Private
    let comp2 = CompositeProof::compose(
        transparent.clone(),
        vec![private.clone()],
    );
    assert_eq!(comp2.privacy_mode, PrivacyMode::Private);
    assert!(comp2.verify_composition());

    // PrivateInputs + Private = Private
    let comp3 = CompositeProof::compose(
        private_inputs.clone(),
        vec![private.clone()],
    );
    assert_eq!(comp3.privacy_mode, PrivacyMode::Private);
    assert!(comp3.verify_composition());

    // All three: Transparent + PrivateInputs + Private = Private
    let comp4 = CompositeProof::compose(
        transparent.clone(),
        vec![private_inputs.clone(), private.clone()],
    );
    assert_eq!(comp4.privacy_mode, PrivacyMode::Private);
    assert!(comp4.verify_composition());
}

// ========================================================================
// V14-38: VerifiedResponse from_bytes rejects truncated proof section
// ========================================================================

/// Craft a wire message where proof_len claims N bytes but fewer are available.
#[test]
fn v14_38_response_truncated_proof() {
    let proof = VerifiedProof::Mock {
        input_hash: ZERO_HASH,
        output_hash: ZERO_HASH,
        privacy_mode: PrivacyMode::Transparent,
    };
    let response = VerifiedResponse::new(&proof, ZERO_HASH, b"data".to_vec(), ZERO_HASH);
    let bytes = response.to_bytes();

    // Truncate: remove last 10 bytes
    let _truncated = &bytes[..bytes.len() - 10];
    // Depending on where the truncation falls, this should fail
    // Let's truncate more aggressively: only keep header + oversized proof_len
    let mut crafted = Vec::new();
    crafted.extend_from_slice(&bytes[..98]);  // header
    crafted.extend_from_slice(&999999u32.to_be_bytes()); // proof_len = 999999
    crafted.extend_from_slice(&[0xAA; 10]);   // only 10 bytes of "proof"

    let result = VerifiedResponse::from_bytes(&crafted);
    assert!(
        result.is_err(),
        "V14-38: Truncated proof section must be rejected"
    );
}

// ========================================================================
// V14-39: HashIvc verify with swapped I/O hashes
// ========================================================================

/// Swapping input_hash and output_hash should cause verification failure.
#[test]
fn v14_39_hashivc_swapped_io() {
    let ivc = HashIvc;
    let code_hash = hash_data(b"func");
    let mut acc = ivc.init(&code_hash, PrivacyMode::Transparent);
    let w = StepWitness {
        state_before: hash_data(b"b"),
        state_after: hash_data(b"a"),
        step_inputs: hash_data(b"i"),
    };
    ivc.fold_step(&mut acc, &w).unwrap();
    let input_hash = hash_data(b"specific_input");
    let output_hash = hash_data(b"specific_output");
    acc.input_hash = input_hash;
    acc.output_hash = output_hash;
    let proof = ivc.finalize(acc).unwrap();

    // Correct: passes
    assert!(ivc.verify(&proof, &input_hash, &output_hash).unwrap());

    // Swapped: fails
    assert!(
        !ivc.verify(&proof, &output_hash, &input_hash).unwrap(),
        "V14-39: Swapped I/O hashes must cause verification failure"
    );
}

// ========================================================================
// V14-40: Disclosure Merkle proof leaf_index cross-check
// ========================================================================

/// Verify that verify_disclosure checks leaf_index matches the token position.
#[test]
fn v14_40_disclosure_leaf_index_cross_check() {
    let tokens = vec![10, 20, 30, 40];
    let verified = make_verified(tokens);

    // Reveal position 1
    let mut disclosure = create_disclosure(&verified, &[1]).unwrap();

    // Tamper: change the proof's leaf_index to 2 (wrong position)
    disclosure.proofs[0].leaf_index = 2;

    assert!(
        !verify_disclosure(&disclosure),
        "V14-40: Wrong leaf_index in Merkle proof must be rejected"
    );
}

// ========================================================================
// V14-41: FixedPoint comparison edge cases around zero
// ========================================================================

/// Verify that FixedPoint comparison works correctly for values straddling zero.
#[test]
fn v14_41_fixedpoint_zero_crossing() {
    let tiny_pos = FixedPoint::from_raw(1);    // smallest positive
    let tiny_neg = FixedPoint::from_raw(-1);   // smallest negative
    let zero = FixedPoint::ZERO;

    assert!(tiny_pos > zero);
    assert!(tiny_neg < zero);
    assert!(tiny_pos > tiny_neg);
    assert_ne!(tiny_pos, tiny_neg);
    assert_ne!(tiny_pos, zero);
    assert_ne!(tiny_neg, zero);

    // But to_i64 shows all three as 0
    assert_eq!(tiny_pos.to_i64(), 0);
    assert_eq!(tiny_neg.to_i64(), 0);
    assert_eq!(zero.to_i64(), 0);

    // This means two values that compare differently display as the same integer
    assert_eq!(
        tiny_pos.to_i64(),
        tiny_neg.to_i64(),
        "V14-41: Opposite-sign sub-unit values show as same integer"
    );
}

// ========================================================================
// V14-42: CodeAttestation sign_message deterministic
// ========================================================================

/// Verify that sign_message produces identical output for same inputs.
#[test]
fn v14_42_attestation_sign_message_deterministic() {
    let att = CodeAttestation {
        node_id: [0xAA; 32],
        code_hash: [0xBB; 32],
        circuit_id: 42,
        signature: [0x00; 64], // signature doesn't affect sign_message
    };

    let msg1 = att.sign_message();
    let msg2 = att.sign_message();
    assert_eq!(msg1, msg2, "V14-42: sign_message is deterministic");

    // Changing the signature should NOT affect sign_message
    let att2 = CodeAttestation {
        signature: [0xFF; 64],
        ..att.clone()
    };
    assert_eq!(
        att.sign_message(),
        att2.sign_message(),
        "V14-42: sign_message independent of signature field"
    );

    // But changing node_id DOES affect it
    let att3 = CodeAttestation {
        node_id: [0xCC; 32],
        ..att.clone()
    };
    assert_ne!(att.sign_message(), att3.sign_message());

    // Changing circuit_id affects it
    let att4 = CodeAttestation {
        circuit_id: 43,
        ..att.clone()
    };
    assert_ne!(att.sign_message(), att4.sign_message());
}
