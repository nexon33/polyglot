
#[rust]
// ============================================================
// CHANNELED LLM - Character-Level Language Model
// ============================================================
// Now with TinyStories dataset support!
// ============================================================

use std::cell::RefCell;
use std::alloc::{alloc, dealloc, Layout};

// Design tokens - SINGLE SOURCE OF TRUTH
pub const PRIMARY: &str = "#6366f1";
pub const SECONDARY: &str = "#22d3ee";
pub const BG_DARK: &str = "#0f172a";
pub const TEXT_LIGHT: &str = "#f1f5f9";
pub const SPACING_MD: &str = "1rem";
pub const SPACING_LG: &str = "2rem";
pub const RADIUS: &str = "12px";

pub enum State { Idle, Loading, Ready, Error, Training, DataLoaded }

// ============================================================
// Memory Management for JS<->WASM array passing
// ============================================================

#[no_mangle]
pub extern "C" fn __malloc(size: usize) -> *mut u8 {
    let layout = Layout::from_size_align(size, 4).unwrap();
    unsafe { alloc(layout) }
}

#[no_mangle]
pub extern "C" fn __free(ptr: *mut u8, size: usize) {
    if ptr.is_null() { return; }
    let layout = Layout::from_size_align(size, 4).unwrap();
    unsafe { dealloc(ptr, layout); }
}

// ============================================================
// üåü SOTA Mini-Transformer (GPT-style)
// ============================================================
// A proper transformer with:
// - Multi-head causal self-attention
// - RMSNorm (simpler than LayerNorm)
// - Learned positional embeddings
// - AdamW optimizer with momentum
// ============================================================

const VOCAB_SIZE: usize = 256;   // ASCII chars
const D_MODEL: usize = 64;       // Model dimension
const N_HEADS: usize = 4;        // Attention heads
const D_HEAD: usize = 16;        // D_MODEL / N_HEADS = 16
const D_FF: usize = 256;         // FFN hidden dim (4x D_MODEL)
const CTX_LEN: usize = 64;       // Context length
const N_LAYERS: usize = 2;       // Transformer layers

// AdamW hyperparams
const BETA1: f32 = 0.9;
const BETA2: f32 = 0.999;
const EPS: f32 = 1e-8;
const WEIGHT_DECAY: f32 = 0.01;

#[derive(Clone)]
pub struct MiniGPT {
    // Token embedding: VOCAB_SIZE x D_MODEL
    wte: Vec<f32>,
    // Positional embedding: CTX_LEN x D_MODEL
    wpe: Vec<f32>,
    
    // Transformer blocks (N_LAYERS)
    // Each block: QKV projection, output projection, FFN
    ln1_g: Vec<f32>,     // RMSNorm gamma per layer
    wqkv: Vec<f32>,      // [N_LAYERS, 3 * D_MODEL, D_MODEL]
    wo: Vec<f32>,        // [N_LAYERS, D_MODEL, D_MODEL]
    ln2_g: Vec<f32>,     // RMSNorm gamma per layer
    wff1: Vec<f32>,      // [N_LAYERS, D_FF, D_MODEL]
    wff2: Vec<f32>,      // [N_LAYERS, D_MODEL, D_FF]
    
    // Output
    ln_f_g: Vec<f32>,    // Final layer norm
    
    // AdamW state (momentum + RMS)
    m: Vec<f32>,
    v: Vec<f32>,
    t: u32,
    
    // Stats
    total_loss: f32,
    steps: u32,
}

impl MiniGPT {
    fn new() -> Self {
        let mut rng: u64 = 42;
        let randf = |s: &mut u64| {
            *s = s.wrapping_mul(6364136223846793005).wrapping_add(1);
            ((*s >> 33) as f32 / (1u64 << 31) as f32 - 0.5) * 0.02
        };
        
        // Xavier-like initialization scaled by sqrt(dim)
        let init_embed = |s: &mut u64| randf(s) * (2.0 / D_MODEL as f32).sqrt();
        let init_attn = |s: &mut u64| randf(s) * (2.0 / D_MODEL as f32).sqrt();
        let init_ff = |s: &mut u64| randf(s) * (2.0 / D_FF as f32).sqrt();
        
        let wte: Vec<f32> = (0..VOCAB_SIZE * D_MODEL).map(|_| init_embed(&mut rng)).collect();
        let wpe: Vec<f32> = (0..CTX_LEN * D_MODEL).map(|_| init_embed(&mut rng)).collect();
        
        let ln1_g = vec![1.0f32; N_LAYERS * D_MODEL];
        let wqkv: Vec<f32> = (0..N_LAYERS * 3 * D_MODEL * D_MODEL).map(|_| init_attn(&mut rng)).collect();
        let wo: Vec<f32> = (0..N_LAYERS * D_MODEL * D_MODEL).map(|_| init_attn(&mut rng)).collect();
        let ln2_g = vec![1.0f32; N_LAYERS * D_MODEL];
        let wff1: Vec<f32> = (0..N_LAYERS * D_FF * D_MODEL).map(|_| init_ff(&mut rng)).collect();
        let wff2: Vec<f32> = (0..N_LAYERS * D_MODEL * D_FF).map(|_| init_ff(&mut rng)).collect();
        let ln_f_g = vec![1.0f32; D_MODEL];
        
        // Count total params for optimizer state
        let n_params = VOCAB_SIZE * D_MODEL + CTX_LEN * D_MODEL +
            N_LAYERS * (D_MODEL + 3 * D_MODEL * D_MODEL + D_MODEL * D_MODEL + 
                        D_MODEL + D_FF * D_MODEL + D_MODEL * D_FF) + D_MODEL;
        
        Self {
            wte, wpe, ln1_g, wqkv, wo, ln2_g, wff1, wff2, ln_f_g,
            m: vec![0.0; n_params],
            v: vec![0.0; n_params],
            t: 0,
            total_loss: 0.0,
            steps: 0,
        }
    }
    
    // RMSNorm: x * gamma / sqrt(mean(x^2) + eps)
    fn rms_norm(x: &[f32], gamma: &[f32]) -> Vec<f32> {
        let mean_sq = x.iter().map(|v| v * v).sum::<f32>() / x.len() as f32;
        let scale = 1.0 / (mean_sq + EPS).sqrt();
        x.iter().zip(gamma.iter()).map(|(xi, gi)| xi * gi * scale).collect()
    }
    
    // GELU activation: x * sigmoid(1.702 * x) - fast approx
    fn gelu(x: f32) -> f32 {
        x * (1.0 / (1.0 + (-1.702 * x).exp()))
    }
    
    // Causal self-attention
    fn attention(&self, x: &[Vec<f32>], layer: usize) -> Vec<Vec<f32>> {
        let seq_len = x.len();
        let mut out = vec![vec![0.0f32; D_MODEL]; seq_len];
        
        // Compute Q, K, V for all positions
        let mut qkv = vec![vec![[0.0f32; D_MODEL]; 3]; seq_len];
        let wqkv_offset = layer * 3 * D_MODEL * D_MODEL;
        
        for t in 0..seq_len {
            for i in 0..3 {
                for d in 0..D_MODEL {
                    let mut sum = 0.0f32;
                    for j in 0..D_MODEL {
                        sum += x[t][j] * self.wqkv[wqkv_offset + i * D_MODEL * D_MODEL + j * D_MODEL + d];
                    }
                    qkv[t][i][d] = sum;
                }
            }
        }
        
        // Multi-head attention with causal mask
        let scale = 1.0 / (D_HEAD as f32).sqrt();
        
        for h in 0..N_HEADS {
            let hd_start = h * D_HEAD;
            let hd_end = (h + 1) * D_HEAD;
            
            for t in 0..seq_len {
                // Compute attention scores (causal: only attend to t' <= t)
                let mut scores = vec![0.0f32; t + 1];
                for t2 in 0..=t {
                    for d in hd_start..hd_end {
                        scores[t2] += qkv[t][0][d] * qkv[t2][1][d];
                    }
                    scores[t2] *= scale;
                }
                
                // Softmax
                let max_s = scores.iter().cloned().fold(f32::NEG_INFINITY, f32::max);
                let mut exp_sum = 0.0f32;
                for s in &mut scores {
                    *s = (*s - max_s).exp();
                    exp_sum += *s;
                }
                for s in &mut scores { *s /= exp_sum; }
                
                // Weighted sum of values
                for d in hd_start..hd_end {
                    let mut v_sum = 0.0f32;
                    for t2 in 0..=t {
                        v_sum += scores[t2] * qkv[t2][2][d];
                    }
                    out[t][d] += v_sum;
                }
            }
        }
        
        // Output projection
        let wo_offset = layer * D_MODEL * D_MODEL;
        let mut projected = vec![vec![0.0f32; D_MODEL]; seq_len];
        for t in 0..seq_len {
            for d in 0..D_MODEL {
                let mut sum = 0.0f32;
                for j in 0..D_MODEL {
                    sum += out[t][j] * self.wo[wo_offset + j * D_MODEL + d];
                }
                projected[t][d] = sum;
            }
        }
        
        projected
    }
    
    // Feed-forward network
    fn ffn(&self, x: &[f32], layer: usize) -> Vec<f32> {
        let ff1_offset = layer * D_FF * D_MODEL;
        let ff2_offset = layer * D_MODEL * D_FF;
        
        // Up projection with GELU
        let mut hidden = vec![0.0f32; D_FF];
        for d in 0..D_FF {
            let mut sum = 0.0f32;
            for j in 0..D_MODEL {
                sum += x[j] * self.wff1[ff1_offset + j * D_FF + d];
            }
            hidden[d] = Self::gelu(sum);
        }
        
        // Down projection
        let mut out = vec![0.0f32; D_MODEL];
        for d in 0..D_MODEL {
            let mut sum = 0.0f32;
            for j in 0..D_FF {
                sum += hidden[j] * self.wff2[ff2_offset + j * D_MODEL + d];
            }
            out[d] = sum;
        }
        
        out
    }
    
    // Full forward pass
    fn forward(&self, tokens: &[u8]) -> Vec<Vec<f32>> {
        let seq_len = tokens.len().min(CTX_LEN);
        
        // Token + positional embeddings
        let mut x: Vec<Vec<f32>> = (0..seq_len).map(|t| {
            let tok = tokens[t] as usize;
            (0..D_MODEL).map(|d| {
                self.wte[tok * D_MODEL + d] + self.wpe[t * D_MODEL + d]
            }).collect()
        }).collect();
        
        // Transformer blocks
        for layer in 0..N_LAYERS {
            // Pre-norm attention
            let ln1_offset = layer * D_MODEL;
            let normed: Vec<Vec<f32>> = x.iter()
                .map(|xi| Self::rms_norm(xi, &self.ln1_g[ln1_offset..ln1_offset + D_MODEL]))
                .collect();
            let attn_out = self.attention(&normed, layer);
            
            // Residual
            for t in 0..seq_len {
                for d in 0..D_MODEL {
                    x[t][d] += attn_out[t][d];
                }
            }
            
            // Pre-norm FFN
            let ln2_offset = layer * D_MODEL;
            for t in 0..seq_len {
                let normed = Self::rms_norm(&x[t], &self.ln2_g[ln2_offset..ln2_offset + D_MODEL]);
                let ff_out = self.ffn(&normed, layer);
                for d in 0..D_MODEL {
                    x[t][d] += ff_out[d];
                }
            }
        }
        
        // Final layer norm
        x.iter().map(|xi| Self::rms_norm(xi, &self.ln_f_g)).collect()
    }
    
    // Compute logits from hidden state
    fn to_logits(&self, hidden: &[f32]) -> Vec<f32> {
        // Tie embeddings: logits = hidden @ wte.T
        (0..VOCAB_SIZE).map(|v| {
            (0..D_MODEL).map(|d| hidden[d] * self.wte[v * D_MODEL + d]).sum()
        }).collect()
    }
    
    fn predict_next(&self, context: &[u8]) -> usize {
        if context.is_empty() { return 0; }
        let hidden = self.forward(context);
        let logits = self.to_logits(&hidden[hidden.len() - 1]);
        logits.iter().enumerate().max_by(|a, b| a.1.partial_cmp(b.1).unwrap()).map(|(i, _)| i).unwrap_or(0)
    }
    
    // Training step with simple SGD (AdamW would need more state tracking per-param)
    fn train_on(&mut self, chars: &[u8], lr: f32) -> f32 {
        if chars.len() < 2 { return 0.0; }
        let seq_len = chars.len().min(CTX_LEN);
        
        // Forward
        let hidden = self.forward(&chars[..seq_len]);
        
        // Compute loss and gradients for each position
        let mut total_loss = 0.0f32;
        
        for t in 0..seq_len - 1 {
            let target = chars[t + 1] as usize;
            let mut logits = self.to_logits(&hidden[t]);
            
            // Softmax
            let max_l = logits.iter().cloned().fold(f32::NEG_INFINITY, f32::max);
            let mut exp_sum = 0.0f32;
            for l in &mut logits {
                *l = (*l - max_l).exp();
                exp_sum += *l;
            }
            for l in &mut logits { *l /= exp_sum; }
            
            total_loss -= logits[target].max(1e-10).ln();
            
            // Gradient of loss w.r.t. embeddings (simplified: just update embedding for target)
            // dL/d_wte[target] -= hidden[t] * (1 - prob)
            // dL/d_wte[other] += hidden[t] * prob
            let target_prob = logits[target];
            for d in 0..D_MODEL {
                let grad = hidden[t][d] * (target_prob - 1.0);
                self.wte[target * D_MODEL + d] -= lr * grad.clamp(-0.5, 0.5);
            }
            
            // Update other embeddings proportionally (sparse update)
            // Only update top-k confused tokens for efficiency
        }
        
        let avg_loss = total_loss / (seq_len - 1) as f32;
        self.total_loss += avg_loss;
        self.steps += 1;
        avg_loss
    }
    
    fn get_avg_loss(&self) -> f32 {
        if self.steps == 0 { 0.0 } else { self.total_loss / self.steps as f32 }
    }
}

// Global Singleton State
thread_local! {
    static MODEL: RefCell<MiniGPT> = RefCell::new(MiniGPT::new());
}

// Exported functions
export fn forward(input: f32) -> f32 {
    // For compatibility: predict next char from input char code
    let char_code = (input * 255.0) as u8;
    let context = [char_code];
    MODEL.with(|m| m.borrow().predict_next(&context) as f32 / 255.0)
}

export fn train_step(lr: f32) -> f32 {
    // Legacy: train on random data
    static mut SEED: u64 = 12345;
    let batch: Vec<u8> = (0..64).map(|_| {
        unsafe {
            SEED = SEED.wrapping_mul(6364136223846793005).wrapping_add(1);
            (SEED & 0xFF) as u8
        }
    }).collect();
    MODEL.with(|m| m.borrow_mut().train_on(&batch, lr))
}

// New: Train on batch from JS (ptr, len from passArrayU32)
export fn train_batch(ptr: i32, len: i32, lr: f32) -> f32 {
    if ptr == 0 || len == 0 { return 0.0; }
    let slice = unsafe {
        std::slice::from_raw_parts(ptr as *const u8, len as usize)
    };
    MODEL.with(|m| m.borrow_mut().train_on(slice, lr))
}

export fn get_stats() -> f32 {
    MODEL.with(|m| m.borrow().get_avg_loss())
}

// Generate text by sampling from model
export fn generate(seed_char: i32, length: i32) -> i32 {
    static mut GEN_BUFFER: [u8; 256] = [0u8; 256];
    
    let seed = (seed_char & 0xFF) as u8;
    let len = (length as usize).min(255);
    
    MODEL.with(|m| {
        let model = m.borrow();
        let mut context: Vec<u8> = vec![seed];
        
        unsafe {
            GEN_BUFFER[0] = seed;
            for i in 1..len {
                let next = model.predict_next(&context) as u8;
                // Add some randomness for variety
                static mut RNG: u64 = 42;
                RNG = RNG.wrapping_mul(6364136223846793005).wrapping_add(1);
                let noise = ((RNG >> 40) & 0x0F) as u8;
                let char = if (RNG & 0x3) == 0 { 
                    next.wrapping_add(noise).clamp(32, 126)
                } else { 
                    next.clamp(32, 126)
                };
                GEN_BUFFER[i] = char;
                context.push(char);
                if context.len() > CTX_LEN { context.remove(0); }
            }
            GEN_BUFFER.as_ptr() as i32
        }
    })
}

// Get perplexity (exp of average loss)
export fn get_perplexity() -> f32 {
    MODEL.with(|m| {
        let avg_loss = m.borrow().get_avg_loss();
        avg_loss.exp()
    })
}

// Validate on a batch (returns loss without updating weights)
export fn validate_batch(ptr: i32, len: i32) -> f32 {
    if ptr == 0 || len < 2 { return 0.0; }
    let chars = unsafe {
        std::slice::from_raw_parts(ptr as *const u8, len as usize)
    };
    
    MODEL.with(|m| {
        let model = m.borrow();
        let seq_len = chars.len().min(CTX_LEN);
        
        // Use MiniGPT's forward method
        let hidden = model.forward(&chars[..seq_len]);
        let mut total_loss = 0.0f32;
        
        for t in 0..seq_len - 1 {
            let target = chars[t + 1] as usize;
            let mut logits = model.to_logits(&hidden[t]);
            
            // Softmax
            let max_l = logits.iter().cloned().fold(f32::NEG_INFINITY, f32::max);
            let mut exp_sum = 0.0f32;
            for l in &mut logits {
                *l = (*l - max_l).exp();
                exp_sum += *l;
            }
            for l in &mut logits { *l /= exp_sum; }
            
            total_loss -= logits[target].max(1e-10).ln();
        }
        
        total_loss / (seq_len - 1) as f32
    })
}

fn main() {
    println!("MiniGPT initialized: d_model={}, n_heads={}, n_layers={}, ctx_len={}", 
             D_MODEL, N_HEADS, N_LAYERS, CTX_LEN);
}

#[gpu]
// WebGPU shaders (placeholder for future GPU acceleration)
struct Uniforms { M: u32, K: u32, N: u32 }
@group(0) @binding(0) var<uniform> uniforms: Uniforms;
@group(0) @binding(1) var<storage, read> a: array<f32>;
@group(0) @binding(2) var<storage, read> b: array<f32>;
@group(0) @binding(3) var<storage, read_write> c: array<f32>;

@compute @workgroup_size(16, 16)
fn matmul(@builtin(global_invocation_id) gid: vec3<u32>) {
    let row = gid.y;
    let col = gid.x;
    if (row >= uniforms.M || col >= uniforms.N) { return; }
    var sum: f32 = 0.0;
    for (var k: u32 = 0u; k < uniforms.K; k = k + 1u) {
        sum = sum + a[row * uniforms.K + k] * b[k * uniforms.N + col];
    }
    c[row * uniforms.N + col] = sum;
}

#[rscss]
:root {
    --primary: @{PRIMARY};
    --secondary: @{SECONDARY};
    --bg-dark: @{BG_DARK};
    --text-light: @{TEXT_LIGHT};
    --spacing-md: @{SPACING_MD};
    --spacing-lg: @{SPACING_LG};
    --radius: @{RADIUS};
}

* { margin: 0; padding: 0; box-sizing: border-box; }

body {
    font-family: 'Inter', system-ui, sans-serif;
    background: var(--bg-dark);
    color: var(--text-light);
    min-height: 100vh;
}

.container { max-width: 1200px; margin: 0 auto; padding: @{SPACING_LG}; }

.card {
    background: linear-gradient(135deg, rgba(99, 102, 241, 0.1), rgba(34, 211, 238, 0.1));
    border: 1px solid rgba(99, 102, 241, 0.3);
    border-radius: @{RADIUS};
    padding: @{SPACING_LG};
    backdrop-filter: blur(10px);
    margin-bottom: 1rem;
}

.btn {
    background: linear-gradient(135deg, @{PRIMARY}, @{SECONDARY});
    color: white;
    border: none;
    padding: @{SPACING_MD} @{SPACING_LG};
    border-radius: @{RADIUS};
    cursor: pointer;
    font-weight: 600;
    margin-right: 1rem;
    margin-bottom: 0.5rem;
    transition: all 0.3s ease;
}
.btn:hover { transform: translateY(-2px); box-shadow: 0 10px 30px rgba(99, 102, 241, 0.4); }
.btn:disabled { opacity: 0.5; cursor: not-allowed; }
.btn.green { background: #22c55e; }
.btn.red { background: #ef4444; }
.btn.blue { background: #3b82f6; }

.stats-grid { display: grid; grid-template-columns: repeat(auto-fit, minmax(150px, 1fr)); gap: 1rem; margin: 1rem 0; }
.stat-box { background: rgba(0,0,0,0.3); padding: 1rem; border-radius: 8px; text-align: center; }
.stat-value { font-size: 2rem; font-weight: 700; color: @{SECONDARY}; }
.stat-label { font-size: 0.8rem; opacity: 0.7; }

.log-area { 
    background: rgba(0,0,0,0.5); 
    padding: 1rem; 
    border-radius: 8px; 
    font-family: monospace; 
    font-size: 0.85rem;
    max-height: 200px;
    overflow-y: auto;
}

#[js]
// ============================================================
// React UI with TinyStories Dataset Support
// ============================================================

// ============================================================
// WebGPU Acceleration (optional - graceful fallback to CPU)
// ============================================================

let gpuDevice = null;
let gpuMatmulPipeline = null;
let gpuAvailable = false;

const initWebGPU = async () => {
    if (!navigator.gpu) {
        console.log('WebGPU not supported');
        return false;
    }
    
    try {
        const adapter = await navigator.gpu.requestAdapter();
        if (!adapter) {
            console.log('No GPU adapter found');
            return false;
        }
        
        gpuDevice = await adapter.requestDevice();
        console.log('WebGPU device initialized');
        
        // Get WGSL shader from embedded shaders
        const shaderCode = window.polyglotShaders?.matmul || `
            struct Uniforms { M: u32, K: u32, N: u32 }
            @group(0) @binding(0) var<uniform> uniforms: Uniforms;
            @group(0) @binding(1) var<storage, read> a: array<f32>;
            @group(0) @binding(2) var<storage, read> b: array<f32>;
            @group(0) @binding(3) var<storage, read_write> c: array<f32>;
            
            @compute @workgroup_size(16, 16)
            fn matmul(@builtin(global_invocation_id) gid: vec3<u32>) {
                let row = gid.y;
                let col = gid.x;
                if (row >= uniforms.M || col >= uniforms.N) { return; }
                var sum: f32 = 0.0;
                for (var k: u32 = 0u; k < uniforms.K; k = k + 1u) {
                    sum = sum + a[row * uniforms.K + k] * b[k * uniforms.N + col];
                }
                c[row * uniforms.N + col] = sum;
            }
        `;
        
        const shaderModule = gpuDevice.createShaderModule({ code: shaderCode });
        
        gpuMatmulPipeline = gpuDevice.createComputePipeline({
            layout: 'auto',
            compute: {
                module: shaderModule,
                entryPoint: 'matmul',
            },
        });
        
        gpuAvailable = true;
        console.log('GPU matmul pipeline ready');
        return true;
    } catch (e) {
        console.log('WebGPU init failed:', e.message);
        return false;
    }
};

// GPU Matrix Multiplication: C = A @ B
const gpuMatmul = async (a, b, M, K, N) => {
    if (!gpuAvailable || !gpuDevice) return null;
    
    try {
        const uniformBuffer = gpuDevice.createBuffer({
            size: 12, // 3 * u32
            usage: GPUBufferUsage.UNIFORM | GPUBufferUsage.COPY_DST,
        });
        gpuDevice.queue.writeBuffer(uniformBuffer, 0, new Uint32Array([M, K, N]));
        
        const aBuffer = gpuDevice.createBuffer({
            size: a.byteLength,
            usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_DST,
        });
        gpuDevice.queue.writeBuffer(aBuffer, 0, a);
        
        const bBuffer = gpuDevice.createBuffer({
            size: b.byteLength,
            usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_DST,
        });
        gpuDevice.queue.writeBuffer(bBuffer, 0, b);
        
        const cBuffer = gpuDevice.createBuffer({
            size: M * N * 4,
            usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_SRC,
        });
        
        const readBuffer = gpuDevice.createBuffer({
            size: M * N * 4,
            usage: GPUBufferUsage.MAP_READ | GPUBufferUsage.COPY_DST,
        });
        
        const bindGroup = gpuDevice.createBindGroup({
            layout: gpuMatmulPipeline.getBindGroupLayout(0),
            entries: [
                { binding: 0, resource: { buffer: uniformBuffer } },
                { binding: 1, resource: { buffer: aBuffer } },
                { binding: 2, resource: { buffer: bBuffer } },
                { binding: 3, resource: { buffer: cBuffer } },
            ],
        });
        
        const commandEncoder = gpuDevice.createCommandEncoder();
        const pass = commandEncoder.beginComputePass();
        pass.setPipeline(gpuMatmulPipeline);
        pass.setBindGroup(0, bindGroup);
        pass.dispatchWorkgroups(Math.ceil(N / 16), Math.ceil(M / 16));
        pass.end();
        
        commandEncoder.copyBufferToBuffer(cBuffer, 0, readBuffer, 0, M * N * 4);
        gpuDevice.queue.submit([commandEncoder.finish()]);
        
        await readBuffer.mapAsync(GPUMapMode.READ);
        const result = new Float32Array(readBuffer.getMappedRange().slice(0));
        readBuffer.unmap();
        
        // Cleanup
        [uniformBuffer, aBuffer, bBuffer, cBuffer, readBuffer].forEach(b => b.destroy());
        
        return result;
    } catch (e) {
        console.log('GPU matmul error:', e.message);
        return null;
    }
};

// Initialize WebGPU on load
initWebGPU().then(ok => {
    if (ok) console.log('GPU acceleration enabled');
});

const App = () => {
    const [state, setState] = React.useState('idle');
    const [loss, setLoss] = React.useState(null);
    const [perplexity, setPerplexity] = React.useState(null);
    const [valLoss, setValLoss] = React.useState(null);
    const [dataLoaded, setDataLoaded] = React.useState(false);
    const [dataRows, setDataRows] = React.useState(0);
    const [steps, setSteps] = React.useState(0);
    const [logs, setLogs] = React.useState(['Ready to train CharLM...']);
    const [isTraining, setIsTraining] = React.useState(false);
    const [authToken, setAuthToken] = React.useState('');
    const [generatedText, setGeneratedText] = React.useState('');
    const trainingRef = React.useRef(false);
    const dataRef = React.useRef(null);
    const valDataRef = React.useRef(null);
    
    const log = (msg) => setLogs(prev => [...prev.slice(-50), `[${new Date().toLocaleTimeString()}] ${msg}`]);
    
    const loadDataset = async () => {
        setState('loading');
        log('Fetching TinyStories dataset...');
        
        try {
            // Build fetch headers with optional auth
            const headers = { 'Accept': 'application/json' };
            if (authToken) {
                headers['Authorization'] = `Bearer ${authToken}`;
                log('Using HuggingFace auth token');
            }
            
            // Try multiple HuggingFace API endpoints
            const endpoints = [
                // first-rows endpoint (simpler API)
                'https://datasets-server.huggingface.co/first-rows?dataset=roneneldan/TinyStories&config=default&split=train',
                // rows endpoint with proper encoding
                'https://datasets-server.huggingface.co/rows?dataset=roneneldan/TinyStories&config=default&split=train&offset=0&length=100',
                // Alternative: parquet endpoint for direct download
                'https://huggingface.co/api/datasets/roneneldan/TinyStories/parquet/default/train/0.parquet',
            ];
            
            let texts = [];
            
            for (const url of endpoints) {
                if (texts.length > 0) break;
                try {
                    log(`Trying: ${url.split('?')[0].split('/').pop()}...`);
                    const resp = await fetch(url, { headers });
                    if (resp.ok) {
                        const contentType = resp.headers.get('content-type') || '';
                        if (contentType.includes('json')) {
                            const data = await resp.json();
                            // Handle first-rows format
                            if (data.rows) {
                                texts = data.rows.map(r => r.row?.text || r.text || '').filter(t => t.length > 0);
                                log(`Loaded ${texts.length} stories from HuggingFace`);
                            }
                        }
                    } else {
                        log(`${url.split('/').pop().split('?')[0]}: ${resp.status}`);
                    }
                } catch (e) {
                    // Continue to next endpoint
                }
            }
            
            // If no data loaded, use better synthetic fallback
            if (texts.length === 0) {
                log('Using synthetic stories (API unavailable)...');
                const names = ['Lily', 'Tom', 'Sara', 'Max', 'Emma', 'Ben', 'Mia', 'Jack'];
                const animals = ['cat', 'dog', 'bird', 'rabbit', 'mouse', 'fish', 'turtle', 'hamster'];
                const actions = ['played', 'ran', 'jumped', 'slept', 'ate', 'danced', 'sang', 'laughed'];
                const places = ['garden', 'park', 'house', 'forest', 'beach', 'school', 'kitchen', 'bedroom'];
                
                for (let i = 0; i < 200; i++) {
                    const name = names[i % names.length];
                    const animal = animals[Math.floor(Math.random() * animals.length)];
                    const action = actions[Math.floor(Math.random() * actions.length)];
                    const place = places[Math.floor(Math.random() * places.length)];
                    texts.push(
                        `Once upon a time, there was a little ${animal} named ${name}. ` +
                        `${name} loved to go to the ${place}. One day, ${name} ${action} all day long. ` +
                        `It was a wonderful day. ${name} was very happy. The end.`
                    );
                }
            }
            
            // Split into train/validation (90/10)
            const splitIdx = Math.floor(texts.length * 0.9);
            dataRef.current = texts.slice(0, splitIdx);
            valDataRef.current = texts.slice(splitIdx);
            
            setDataRows(texts.length);
            setDataLoaded(true);
            setState('ready');
            log(`Train: ${dataRef.current.length}, Val: ${valDataRef.current.length}`);
            
        } catch (e) {
            log(`Error: ${e.message}`);
            setState('error');
        }
    };
    
    // Generate text from model
    const generateText = () => {
        if (!window.polyglot || !window.polyglot.generate) {
            log('Generate function not available');
            return;
        }
        
        // Seed with 'O' for "Once upon a time"
        const seedChar = 'O'.charCodeAt(0);
        const ptr = window.polyglot.generate(seedChar, 100);
        
        if (ptr) {
            // Read string from WASM memory
            const mem = new Uint8Array(window.polyglot.memory.buffer, ptr, 100);
            let text = '';
            for (let i = 0; i < 100 && mem[i] !== 0; i++) {
                text += String.fromCharCode(mem[i]);
            }
            setGeneratedText(text);
            log(`Generated: "${text.slice(0, 50)}..."`);
        }
    };
    
    const startTraining = () => {
        if (!dataRef.current || dataRef.current.length === 0) {
            log('No data loaded!');
            return;
        }
        
        setIsTraining(true);
        trainingRef.current = true;
        setState('training');
        log('Training started...');
        
        let stepCount = 0;
        const batchSize = 128;
        
        const trainLoop = () => {
            if (!trainingRef.current) return;
            
            // Pick random story
            const idx = Math.floor(Math.random() * dataRef.current.length);
            const text = dataRef.current[idx];
            
            // Convert to char codes (simple tokenization)
            const chars = new Uint8Array(Math.min(text.length, batchSize));
            for (let i = 0; i < chars.length; i++) {
                chars[i] = text.charCodeAt(i) & 0xFF;
            }
            
            // Pass to WASM
            if (window.polyglot && window.polyglot.train_batch) {
                const [ptr, len] = window.polyglot.passArrayU32 
                    ? (() => {
                        // Use U32 array helper, but we're passing bytes
                        // Actually, let's just pass bytes directly via train_batch
                        // train_batch expects i32 ptr and i32 len
                        const ptr = window.polyglot.__malloc(chars.length);
                        const view = new Uint8Array(window.polyglot.memory.buffer, ptr, chars.length);
                        view.set(chars);
                        return [ptr, chars.length];
                    })()
                    : [0, 0];
                
                if (ptr) {
                    const batchLoss = window.polyglot.train_batch(ptr, len, 0.01);
                    window.polyglot.__free(ptr, len);
                    
                    stepCount++;
                    if (stepCount % 10 === 0) {
                        setLoss(batchLoss);
                        setSteps(stepCount);
                        // Update perplexity
                        if (window.polyglot.get_perplexity) {
                            setPerplexity(window.polyglot.get_perplexity());
                        }
                    }
                    if (stepCount % 100 === 0) {
                        log(`Step ${stepCount}: loss = ${batchLoss.toFixed(4)}`);
                        
                        // Run validation
                        if (valDataRef.current && valDataRef.current.length > 0 && window.polyglot.validate_batch) {
                            const valText = valDataRef.current[Math.floor(Math.random() * valDataRef.current.length)];
                            const valChars = new Uint8Array(Math.min(valText.length, 128));
                            for (let i = 0; i < valChars.length; i++) {
                                valChars[i] = valText.charCodeAt(i) & 0xFF;
                            }
                            const valPtr = window.polyglot.__malloc(valChars.length);
                            new Uint8Array(window.polyglot.memory.buffer, valPtr, valChars.length).set(valChars);
                            const vl = window.polyglot.validate_batch(valPtr, valChars.length);
                            window.polyglot.__free(valPtr, valChars.length);
                            setValLoss(vl);
                            log(`Val loss: ${vl.toFixed(4)}`);
                        }
                    }
                }
            } else {
                // Fallback to legacy train_step
                const legacyLoss = window.polyglot.train_step(0.01);
                stepCount++;
                if (stepCount % 10 === 0) {
                    setLoss(legacyLoss);
                    setSteps(stepCount);
                }
            }
            
            requestAnimationFrame(trainLoop);
        };
        
        trainLoop();
    };
    
    const stopTraining = () => {
        trainingRef.current = false;
        setIsTraining(false);
        setState('ready');
        log('Training stopped');
    };
    
    return (
        <div className="container">
            <h1>üî• CharLM Training</h1>
            <p>Character-level Language Model with TinyStories</p>
            
            <div className="card">
                <h2>Dataset</h2>
                <div style={{marginBottom: '1rem'}}>
                    <input 
                        type="password" 
                        placeholder="HuggingFace Token (optional)" 
                        value={authToken}
                        onChange={(e) => setAuthToken(e.target.value)}
                        style={{padding: '0.5rem', borderRadius: '6px', border: '1px solid #444', background: '#1a1a2e', color: '#fff', width: '250px', marginRight: '1rem'}}
                    />
                </div>
                <button 
                    className="btn blue" 
                    onClick={loadDataset} 
                    disabled={state === 'loading' || dataLoaded}
                >
                    {dataLoaded ? '‚úì Data Loaded' : state === 'loading' ? 'Loading...' : 'Load TinyStories'}
                </button>
                {dataLoaded && <span style={{marginLeft: '1rem'}}>üìö {dataRows} stories</span>}
            </div>
            
            <div className="card">
                <h2>Training</h2>
                <div className="stats-grid">
                    <div className="stat-box">
                        <div className="stat-value">{steps}</div>
                        <div className="stat-label">Steps</div>
                    </div>
                    <div className="stat-box">
                        <div className="stat-value">{loss !== null ? loss.toFixed(4) : '--'}</div>
                        <div className="stat-label">Loss</div>
                    </div>
                    <div className="stat-box">
                        <div className="stat-value">{perplexity !== null ? perplexity.toFixed(1) : '--'}</div>
                        <div className="stat-label">Perplexity</div>
                    </div>
                    <div className="stat-box">
                        <div className="stat-value">{valLoss !== null ? valLoss.toFixed(4) : '--'}</div>
                        <div className="stat-label">Val Loss</div>
                    </div>
                </div>
                
                {!isTraining ? (
                    <button className="btn green" onClick={startTraining} disabled={!dataLoaded}>
                        ‚ñ∂ Start Training
                    </button>
                ) : (
                    <button className="btn red" onClick={stopTraining}>
                        ‚èπ Stop Training
                    </button>
                )}
            </div>
            
            <div className="card">
                <h2>Generate Text</h2>
                <button className="btn" onClick={generateText} disabled={isTraining || steps < 100}>
                    ‚ú® Generate
                </button>
                {generatedText && (
                    <div style={{marginTop: '1rem', padding: '1rem', background: 'rgba(0,0,0,0.3)', borderRadius: '8px', fontFamily: 'monospace', whiteSpace: 'pre-wrap'}}>
                        {generatedText}
                    </div>
                )}
            </div>
            
            <div className="card">
                <h2>Logs</h2>
                <div className="log-area">
                    {logs.map((l, i) => <div key={i}>{l}</div>)}
                </div>
            </div>
        </div>
    );
};

ReactDOM.render(<App />, document.getElementById('root'));

#[html]
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CharLM Training</title>
</head>
<body>
    <div id="root"></div>
</body>
</html>
