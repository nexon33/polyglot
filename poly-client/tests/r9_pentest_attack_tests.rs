//! Round 9 Penetration Test — Attack Tests for poly-client
//!
//! Focus areas:
//! - rns_ct_scalar_mul missing ct.scale validation (R9-VULN-01)
//! - rns_ct_add_scalar_broadcast missing ct.scale validation (R9-VULN-02)
//! - rns_rotate missing ct.scale validation (R9-VULN-03)
//! - rns_rotate missing rotation key digit count validation (R9-VULN-04)
//! - rns_rescale subnormal scale acceptance (R9-VULN-05)
//! - decompress_with_limit unconditional 33MB allocation (R9-VULN-06)
//! - rns_ct_add_plain epsilon-scale silent zeroing (R9-VULN-07)
//! - rns_ct_add/sub missing num_primes mismatch check (R9-VULN-08)

#![cfg(feature = "ckks")]
#![allow(unused_variables)]

use poly_client::ckks::compress::{self, CompressionLevel};
use poly_client::ckks::rns_ckks::*;

use rand::rngs::StdRng;
use rand::SeedableRng;

fn test_rng_seed(seed: u64) -> StdRng {
    StdRng::seed_from_u64(seed)
}

// ═══════════════════════════════════════════════════════════════════════════
// SECTION 1: rns_ct_scalar_mul missing ct.scale validation (HIGH — R9 fix)
// ═══════════════════════════════════════════════════════════════════════════

/// R9-VULN-01: rns_ct_scalar_mul did not validate ct.scale.
/// Every other homomorphic operation (add, sub, mul, rescale, mod_switch,
/// add_scalar_broadcast) validated ct.scale in R6-R8, but scalar_mul was
/// missed. An attacker with a NaN-scale ciphertext could use scalar_mul
/// as a "laundering" step to bypass validation in downstream operations.
#[test]
#[should_panic(expected = "rns_ct_scalar_mul: ct.scale must be finite and positive")]
fn r9_scalar_mul_nan_scale_rejected() {
    let mut rng = test_rng_seed(9001);
    let ctx = RnsCkksContext::new(3);
    let (_s, pk_b, pk_a) = rns_keygen(&ctx, &mut rng);

    let mut ct = rns_encrypt_f64(1.0, &pk_b, &pk_a, &ctx, &mut rng);
    ct.scale = f64::NAN;

    // Pre-R9: this would silently propagate NaN scale
    let _ = rns_ct_scalar_mul(&ct, 2);
}

#[test]
#[should_panic(expected = "rns_ct_scalar_mul: ct.scale must be finite and positive")]
fn r9_scalar_mul_inf_scale_rejected() {
    let mut rng = test_rng_seed(9002);
    let ctx = RnsCkksContext::new(3);
    let (_s, pk_b, pk_a) = rns_keygen(&ctx, &mut rng);

    let mut ct = rns_encrypt_f64(1.0, &pk_b, &pk_a, &ctx, &mut rng);
    ct.scale = f64::INFINITY;

    let _ = rns_ct_scalar_mul(&ct, 3);
}

#[test]
#[should_panic(expected = "rns_ct_scalar_mul: ct.scale must be finite and positive")]
fn r9_scalar_mul_neg_inf_scale_rejected() {
    let mut rng = test_rng_seed(9003);
    let ctx = RnsCkksContext::new(3);
    let (_s, pk_b, pk_a) = rns_keygen(&ctx, &mut rng);

    let mut ct = rns_encrypt_f64(1.0, &pk_b, &pk_a, &ctx, &mut rng);
    ct.scale = f64::NEG_INFINITY;

    let _ = rns_ct_scalar_mul(&ct, -1);
}

#[test]
#[should_panic(expected = "rns_ct_scalar_mul: ct.scale must be finite and positive")]
fn r9_scalar_mul_zero_scale_rejected() {
    let mut rng = test_rng_seed(9004);
    let ctx = RnsCkksContext::new(3);
    let (_s, pk_b, pk_a) = rns_keygen(&ctx, &mut rng);

    let mut ct = rns_encrypt_f64(1.0, &pk_b, &pk_a, &ctx, &mut rng);
    ct.scale = 0.0;

    let _ = rns_ct_scalar_mul(&ct, 5);
}

#[test]
#[should_panic(expected = "rns_ct_scalar_mul: ct.scale must be finite and positive")]
fn r9_scalar_mul_negative_scale_rejected() {
    let mut rng = test_rng_seed(9005);
    let ctx = RnsCkksContext::new(3);
    let (_s, pk_b, pk_a) = rns_keygen(&ctx, &mut rng);

    let mut ct = rns_encrypt_f64(1.0, &pk_b, &pk_a, &ctx, &mut rng);
    ct.scale = -42.0;

    let _ = rns_ct_scalar_mul(&ct, 7);
}

/// Verify legitimate scalar_mul still works after R9 fix.
#[test]
fn r9_scalar_mul_legitimate_still_works() {
    let mut rng = test_rng_seed(9006);
    let ctx = RnsCkksContext::new(3);
    let (s, pk_b, pk_a) = rns_keygen(&ctx, &mut rng);

    let ct = rns_encrypt_f64(3.0, &pk_b, &pk_a, &ctx, &mut rng);
    let ct_scaled = rns_ct_scalar_mul(&ct, 4);
    let dec = rns_decrypt_f64(&ct_scaled, &s, &ctx);

    assert!(
        (dec - 12.0).abs() < 1.0,
        "3 * 4 should be ~12, got {}",
        dec
    );
}

// ═══════════════════════════════════════════════════════════════════════════
// SECTION 2: rns_ct_add_scalar_broadcast missing ct.scale validation
//            (HIGH — R9 fix)
// ═══════════════════════════════════════════════════════════════════════════

/// R9-VULN-02: rns_ct_add_scalar_broadcast only validated the scalar (R6),
/// not ct.scale. A NaN scale gets passed to simd::encode_simd producing
/// garbage coefficients that silently corrupt the ciphertext.
#[test]
#[should_panic(expected = "rns_ct_add_scalar_broadcast: ct.scale must be finite and positive")]
fn r9_add_scalar_broadcast_nan_ct_scale() {
    let mut rng = test_rng_seed(9010);
    let ctx = RnsCkksContext::new(3);
    let (_s, pk_b, pk_a) = rns_keygen(&ctx, &mut rng);

    let mut ct = rns_encrypt_f64(1.0, &pk_b, &pk_a, &ctx, &mut rng);
    ct.scale = f64::NAN;

    // Pre-R9: this silently encoded the scalar with NaN scale
    let _ = rns_ct_add_scalar_broadcast(&ct, 5.0);
}

#[test]
#[should_panic(expected = "rns_ct_add_scalar_broadcast: ct.scale must be finite and positive")]
fn r9_add_scalar_broadcast_inf_ct_scale() {
    let mut rng = test_rng_seed(9011);
    let ctx = RnsCkksContext::new(3);
    let (_s, pk_b, pk_a) = rns_keygen(&ctx, &mut rng);

    let mut ct = rns_encrypt_f64(1.0, &pk_b, &pk_a, &ctx, &mut rng);
    ct.scale = f64::INFINITY;

    let _ = rns_ct_add_scalar_broadcast(&ct, 1.0);
}

#[test]
#[should_panic(expected = "rns_ct_add_scalar_broadcast: ct.scale must be finite and positive")]
fn r9_add_scalar_broadcast_zero_ct_scale() {
    let mut rng = test_rng_seed(9012);
    let ctx = RnsCkksContext::new(3);
    let (_s, pk_b, pk_a) = rns_keygen(&ctx, &mut rng);

    let mut ct = rns_encrypt_f64(1.0, &pk_b, &pk_a, &ctx, &mut rng);
    ct.scale = 0.0;

    let _ = rns_ct_add_scalar_broadcast(&ct, 2.0);
}

#[test]
#[should_panic(expected = "rns_ct_add_scalar_broadcast: ct.scale must be finite and positive")]
fn r9_add_scalar_broadcast_negative_ct_scale() {
    let mut rng = test_rng_seed(9013);
    let ctx = RnsCkksContext::new(3);
    let (_s, pk_b, pk_a) = rns_keygen(&ctx, &mut rng);

    let mut ct = rns_encrypt_f64(1.0, &pk_b, &pk_a, &ctx, &mut rng);
    ct.scale = -1.0;

    let _ = rns_ct_add_scalar_broadcast(&ct, 3.0);
}

/// Verify legitimate add_scalar_broadcast still works after R9 fix.
#[test]
fn r9_add_scalar_broadcast_legitimate_still_works() {
    let mut rng = test_rng_seed(9014);
    let ctx = RnsCkksContext::new(3);
    let (s, pk_b, pk_a) = rns_keygen(&ctx, &mut rng);

    let ct = rns_encrypt_f64(10.0, &pk_b, &pk_a, &ctx, &mut rng);
    let ct_added = rns_ct_add_scalar_broadcast(&ct, 5.0);
    let dec = rns_decrypt_f64(&ct_added, &s, &ctx);

    assert!(
        (dec - 15.0).abs() < 1.0,
        "10 + 5 should be ~15, got {}",
        dec
    );
}

// ═══════════════════════════════════════════════════════════════════════════
// SECTION 3: rns_rotate missing ct.scale validation (HIGH — R9 fix)
// ═══════════════════════════════════════════════════════════════════════════

/// R9-VULN-03: rns_rotate never validated ct.scale. Rotation preserves
/// scale, so a corrupted scale would propagate silently through the
/// rotation and into all downstream operations.
#[test]
#[should_panic(expected = "rns_rotate: ct.scale must be finite and positive")]
fn r9_rotate_nan_scale_rejected() {
    let mut rng = test_rng_seed(9020);
    let ctx = RnsCkksContext::new(3);
    let (s, pk_b, pk_a) = rns_keygen(&ctx, &mut rng);
    let rot_keys = rns_gen_rotation_keys(&s, &[1], &ctx, &mut rng);

    let mut ct = rns_encrypt_simd(&[1.0, 2.0, 3.0, 4.0], &pk_b, &pk_a, &ctx, &mut rng);
    ct.scale = f64::NAN;

    let _ = rns_rotate(&ct, 1, &rot_keys, &ctx);
}

#[test]
#[should_panic(expected = "rns_rotate: ct.scale must be finite and positive")]
fn r9_rotate_inf_scale_rejected() {
    let mut rng = test_rng_seed(9021);
    let ctx = RnsCkksContext::new(3);
    let (s, pk_b, pk_a) = rns_keygen(&ctx, &mut rng);
    let rot_keys = rns_gen_rotation_keys(&s, &[1], &ctx, &mut rng);

    let mut ct = rns_encrypt_simd(&[1.0, 2.0], &pk_b, &pk_a, &ctx, &mut rng);
    ct.scale = f64::INFINITY;

    let _ = rns_rotate(&ct, 1, &rot_keys, &ctx);
}

#[test]
#[should_panic(expected = "rns_rotate: ct.scale must be finite and positive")]
fn r9_rotate_negative_scale_rejected() {
    let mut rng = test_rng_seed(9022);
    let ctx = RnsCkksContext::new(3);
    let (s, pk_b, pk_a) = rns_keygen(&ctx, &mut rng);
    let rot_keys = rns_gen_rotation_keys(&s, &[1], &ctx, &mut rng);

    let mut ct = rns_encrypt_simd(&[1.0, 2.0], &pk_b, &pk_a, &ctx, &mut rng);
    ct.scale = -100.0;

    let _ = rns_rotate(&ct, 1, &rot_keys, &ctx);
}

/// Verify legitimate rotation still works after R9 fix.
#[test]
fn r9_rotate_legitimate_still_works() {
    let mut rng = test_rng_seed(9023);
    let ctx = RnsCkksContext::new(3);
    let (s, pk_b, pk_a) = rns_keygen(&ctx, &mut rng);
    let rot_keys = rns_gen_rotation_keys(&s, &[1], &ctx, &mut rng);

    let values = vec![10.0, 20.0, 30.0, 40.0];
    let x_rep = replicate_vector(&values, 4);
    let ct = rns_encrypt_simd(&x_rep, &pk_b, &pk_a, &ctx, &mut rng);

    let ct_rot = rns_rotate(&ct, 1, &rot_keys, &ctx);
    let dec = rns_decrypt_simd(&ct_rot, &s, &ctx, 4);

    // Rotation by 1: [10,20,30,40] -> [20,30,40,10]
    assert!(
        (dec[0] - 20.0).abs() < 1.0,
        "slot 0 after rotate by 1: expected ~20, got {}",
        dec[0]
    );
}

// ═══════════════════════════════════════════════════════════════════════════
// SECTION 4: rns_rotate missing rotation key digit count validation
//            (HIGH — R9 fix)
// ═══════════════════════════════════════════════════════════════════════════

/// R9-VULN-04: rns_rotate did not check that the rotation key has enough
/// digit pairs for the ciphertext's decomposition. This mirrors the R8
/// fix for rns_relinearize (eval key digit count). A rotation key generated
/// for fewer primes than the ciphertext would cause an index-out-of-bounds
/// panic deep in the key-switching loop.
#[test]
#[should_panic(expected = "rotation key has")]
fn r9_rotate_key_too_few_digits_rejected() {
    let mut rng = test_rng_seed(9030);

    // Generate rotation key for 3 primes
    let ctx3 = RnsCkksContext::new(3);
    let (s3, pk_b3, pk_a3) = rns_keygen(&ctx3, &mut rng);
    let rot_keys_small = rns_gen_rotation_keys(&s3, &[1], &ctx3, &mut rng);

    // Generate ciphertext at 10 primes (needs many more digits)
    let ctx10 = RnsCkksContext::new(10);
    let (_s10, pk_b10, pk_a10) = rns_keygen(&ctx10, &mut rng);

    let ct = rns_encrypt_simd(&[1.0, 2.0, 3.0, 4.0], &pk_b10, &pk_a10, &ctx10, &mut rng);

    // Try to rotate with undersized rotation key — should panic with R9 check
    let _ = rns_rotate(&ct, 1, &rot_keys_small, &ctx10);
}

/// Verify that rotation key generated at full primes works for rescaled ciphertext.
/// (Rotation key has MORE digits than needed — this is fine.)
#[test]
fn r9_rotate_key_oversized_is_fine() {
    let mut rng = test_rng_seed(9031);

    // Generate rotation key at 5 primes
    let ctx5 = RnsCkksContext::new(5);
    let (s5, pk_b5, pk_a5) = rns_keygen(&ctx5, &mut rng);
    let evk = rns_gen_eval_key(&s5, &ctx5, &mut rng);
    let rot_keys = rns_gen_rotation_keys(&s5, &[1], &ctx5, &mut rng);

    // Encrypt and rescale to fewer primes
    let ct = rns_encrypt_f64(7.0, &pk_b5, &pk_a5, &ctx5, &mut rng);
    let ct2 = rns_encrypt_f64(3.0, &pk_b5, &pk_a5, &ctx5, &mut rng);
    let ct_prod = rns_ct_mul_relin(&ct, &ct2, &evk, &ctx5);
    let ct_rescaled = rns_rescale(&ct_prod);
    assert!(ct_rescaled.c0.num_primes < 5, "should have fewer primes after rescale");

    // Rotate with the full-size rotation key — should work fine
    let ct_rot = rns_rotate(&ct_rescaled, 1, &rot_keys, &ctx5);
    assert_eq!(ct_rot.c0.num_primes, ct_rescaled.c0.num_primes);
}

// ═══════════════════════════════════════════════════════════════════════════
// SECTION 5: rns_rescale subnormal scale acceptance (MEDIUM — R9 fix)
// ═══════════════════════════════════════════════════════════════════════════

/// R9-VULN-05: rns_rescale allowed the resulting scale to be subnormal.
/// A subnormal float passes `> 0.0` and `is_finite()` but has reduced
/// mantissa precision. When used for SIMD decode (dividing coefficients
/// by scale), subnormal scales produce wildly imprecise or Inf results.
///
/// This test crafts a ciphertext with a tiny (but normal) scale such that
/// rescaling would produce a subnormal result.
#[test]
#[should_panic(expected = "resulting scale is subnormal")]
fn r9_rescale_subnormal_result_rejected() {
    let mut rng = test_rng_seed(9040);
    let ctx = RnsCkksContext::new(3);
    let (_s, pk_b, pk_a) = rns_keygen(&ctx, &mut rng);

    let mut ct = rns_encrypt_f64(1.0, &pk_b, &pk_a, &ctx, &mut rng);
    // Set scale to a tiny normal value that will become subnormal after
    // dividing by q_last (~2^36): 1e-300 / 2^36 = ~1.5e-311 which is subnormal.
    ct.scale = 1e-300;

    let _ = rns_rescale(&ct);
}

/// Verify normal rescaling still works after R9 fix.
#[test]
fn r9_rescale_normal_scale_still_works() {
    let mut rng = test_rng_seed(9041);
    let ctx = RnsCkksContext::new(3);
    let (s, pk_b, pk_a) = rns_keygen(&ctx, &mut rng);
    let evk = rns_gen_eval_key(&s, &ctx, &mut rng);

    let ct1 = rns_encrypt_f64(5.0, &pk_b, &pk_a, &ctx, &mut rng);
    let ct2 = rns_encrypt_f64(6.0, &pk_b, &pk_a, &ctx, &mut rng);

    let ct_prod = rns_ct_mul_relin(&ct1, &ct2, &evk, &ctx);
    let ct_rescaled = rns_rescale(&ct_prod);

    let dec = rns_decrypt_f64(&ct_rescaled, &s, &ctx);
    assert!(
        (dec - 30.0).abs() < 1.0,
        "5 * 6 should be ~30, got {}",
        dec
    );
}

// ═══════════════════════════════════════════════════════════════════════════
// SECTION 6: decompress_with_limit unconditional 33MB allocation
//            (MEDIUM — R9 fix)
// ═══════════════════════════════════════════════════════════════════════════

/// R9-VULN-06: decompress_with_limit previously allocated max_bytes + 1
/// (33MB) unconditionally for any payload, even a 100-byte one. The R9 fix
/// uses incremental allocation starting from a small buffer.
///
/// This test verifies that small payloads still decompress correctly.
#[test]
fn r9_compress_small_payload_efficient() {
    let data = vec![42i64; 10];
    let compressed = compress::compress(&data).unwrap();
    let decompressed: Vec<i64> = compress::decompress(&compressed).unwrap();
    assert_eq!(data, decompressed);
}

/// Verify that medium payloads still work with incremental allocation.
#[test]
fn r9_compress_medium_payload_roundtrip() {
    let data: Vec<i64> = (0..4096).collect();

    // V1 (Lossless)
    let c1 = compress::compress(&data).unwrap();
    let d1: Vec<i64> = compress::decompress(&c1).unwrap();
    assert_eq!(data, d1);

    // V2 (Compact)
    let c2 = compress::compress_with(&data, CompressionLevel::Compact).unwrap();
    let d2: Vec<i64> = compress::decompress(&c2).unwrap();
    assert_eq!(data, d2);

    // V2 (Max)
    let c3 = compress::compress_with(&data, CompressionLevel::Max).unwrap();
    let d3: Vec<i64> = compress::decompress(&c3).unwrap();
    assert_eq!(data, d3);
}

/// Large payload (close to the 32MB limit) should still work.
#[test]
fn r9_compress_large_payload_roundtrip() {
    // 8192 i64 values = 64KB — moderate size
    let data: Vec<i64> = (0..8192).map(|i| i * 7 + 3).collect();
    let compressed = compress::compress(&data).unwrap();
    let decompressed: Vec<i64> = compress::decompress(&compressed).unwrap();
    assert_eq!(data, decompressed);
}

/// Verify decompression bomb detection still works with incremental alloc.
#[test]
fn r9_compress_decompression_bomb_still_rejected() {
    let data = vec![42u32; 100];
    let compressed = compress::compress(&data).unwrap();

    // Tamper: set original_size to exceed MAX_DECOMPRESSED_SIZE
    let mut tampered = compressed.clone();
    let fake_size: u32 = (33 * 1024 * 1024) as u32;
    tampered[5..9].copy_from_slice(&fake_size.to_le_bytes());

    let result: Result<Vec<u32>, _> = compress::decompress(&tampered);
    assert!(result.is_err(), "Should still reject fraudulent original_size");
}

// ═══════════════════════════════════════════════════════════════════════════
// SECTION 7: rns_ct_add_plain epsilon-scale silent zeroing
//            (HIGH — R9 fix)
// ═══════════════════════════════════════════════════════════════════════════

/// R9-VULN-07: rns_ct_add_plain with a subnormal ct.scale passes the
/// tolerance check (if delta matches) but simd::encode_simd with a tiny
/// scale rounds all values to zero, silently destroying the addition.
///
/// Attack: craft a ciphertext with scale = 5e-324 (smallest subnormal),
/// call rns_ct_add_plain with matching delta. The plaintext is silently
/// zeroed during encoding.
#[test]
#[should_panic(expected = "rns_ct_add_plain: ct.scale must be finite, positive, and normal")]
fn r9_add_plain_subnormal_scale_rejected() {
    let mut rng = test_rng_seed(9060);
    let ctx = RnsCkksContext::new(3);
    let (_s, pk_b, pk_a) = rns_keygen(&ctx, &mut rng);

    let mut ct = rns_encrypt_f64(1.0, &pk_b, &pk_a, &ctx, &mut rng);
    ct.scale = 5e-324; // smallest subnormal

    // Pre-R9: this would silently encode 42.0 as all zeros
    let _ = rns_ct_add_plain(&ct, 42.0, 5e-324);
}

/// Epsilon-normal scale should also be rejected — while technically normal,
/// it produces all-zero encoding since value * scale rounds to 0 in i64.
#[test]
#[should_panic(expected = "rns_ct_add_plain: ct.scale must be finite, positive, and normal")]
fn r9_add_plain_subnormal_scale_near_zero_rejected() {
    let mut rng = test_rng_seed(9061);
    let ctx = RnsCkksContext::new(3);
    let (_s, pk_b, pk_a) = rns_keygen(&ctx, &mut rng);

    let mut ct = rns_encrypt_f64(1.0, &pk_b, &pk_a, &ctx, &mut rng);
    ct.scale = f64::MIN_POSITIVE / 2.0; // subnormal (half the smallest normal)

    let _ = rns_ct_add_plain(&ct, 1.0, f64::MIN_POSITIVE / 2.0);
}

/// Verify legitimate add_plain still works after R9 fix.
#[test]
fn r9_add_plain_legitimate_still_works() {
    let mut rng = test_rng_seed(9062);
    let ctx = RnsCkksContext::new(3);
    let (s, pk_b, pk_a) = rns_keygen(&ctx, &mut rng);

    let ct = rns_encrypt_f64(10.0, &pk_b, &pk_a, &ctx, &mut rng);
    let ct_added = rns_ct_add_plain(&ct, 5.0, ctx.delta);
    let dec = rns_decrypt_f64(&ct_added, &s, &ctx);

    assert!(
        (dec - 15.0).abs() < 1.0,
        "10 + 5 should be ~15, got {}",
        dec
    );
}

// ═══════════════════════════════════════════════════════════════════════════
// SECTION 8: rns_ct_add/sub missing num_primes mismatch check
//            (MEDIUM — R9 fix)
// ═══════════════════════════════════════════════════════════════════════════

/// R9-VULN-08: rns_ct_add with mismatched num_primes produced an unhelpful
/// "assertion failed" panic from the underlying RnsPoly::add. The R9 fix
/// adds an explicit check with a descriptive error message.
#[test]
#[should_panic(expected = "rns_ct_add: num_primes mismatch")]
fn r9_ct_add_mismatched_num_primes_descriptive_error() {
    let mut rng = test_rng_seed(9070);
    let ctx5 = RnsCkksContext::new(5);
    let ctx3 = RnsCkksContext::new(3);

    let (_s5, pk_b5, pk_a5) = rns_keygen(&ctx5, &mut rng);
    let (_s3, pk_b3, pk_a3) = rns_keygen(&ctx3, &mut rng);

    let ct5 = rns_encrypt_f64(1.0, &pk_b5, &pk_a5, &ctx5, &mut rng);
    let ct3 = rns_encrypt_f64(1.0, &pk_b3, &pk_a3, &ctx3, &mut rng);

    // Forge matching scale and level for the 3-prime ct to bypass other checks
    let mut ct3_forged = ct3;
    ct3_forged.scale = ct5.scale;
    ct3_forged.level = ct5.level;

    // Pre-R9: panics with unhelpful "assertion `left == right` failed"
    // Post-R9: panics with descriptive "num_primes mismatch: a has 5 primes, b has 3 primes"
    let _ = rns_ct_add(&ct5, &ct3_forged);
}

/// Same test for rns_ct_sub.
#[test]
#[should_panic(expected = "rns_ct_sub: num_primes mismatch")]
fn r9_ct_sub_mismatched_num_primes_descriptive_error() {
    let mut rng = test_rng_seed(9071);
    let ctx5 = RnsCkksContext::new(5);
    let ctx3 = RnsCkksContext::new(3);

    let (_s5, pk_b5, pk_a5) = rns_keygen(&ctx5, &mut rng);
    let (_s3, pk_b3, pk_a3) = rns_keygen(&ctx3, &mut rng);

    let ct5 = rns_encrypt_f64(1.0, &pk_b5, &pk_a5, &ctx5, &mut rng);
    let ct3 = rns_encrypt_f64(1.0, &pk_b3, &pk_a3, &ctx3, &mut rng);

    let mut ct3_forged = ct3;
    ct3_forged.scale = ct5.scale;
    ct3_forged.level = ct5.level;

    let _ = rns_ct_sub(&ct5, &ct3_forged);
}

/// Verify that rns_ct_add with matching primes still works.
#[test]
fn r9_ct_add_matching_primes_still_works() {
    let mut rng = test_rng_seed(9072);
    let ctx = RnsCkksContext::new(3);
    let (s, pk_b, pk_a) = rns_keygen(&ctx, &mut rng);

    let ct1 = rns_encrypt_f64(7.0, &pk_b, &pk_a, &ctx, &mut rng);
    let ct2 = rns_encrypt_f64(8.0, &pk_b, &pk_a, &ctx, &mut rng);

    let ct_sum = rns_ct_add(&ct1, &ct2);
    let dec = rns_decrypt_f64(&ct_sum, &s, &ctx);

    assert!(
        (dec - 15.0).abs() < 1.0,
        "7 + 8 should be ~15, got {}",
        dec
    );
}

/// Verify that rns_ct_sub with matching primes still works.
#[test]
fn r9_ct_sub_matching_primes_still_works() {
    let mut rng = test_rng_seed(9073);
    let ctx = RnsCkksContext::new(3);
    let (s, pk_b, pk_a) = rns_keygen(&ctx, &mut rng);

    let ct1 = rns_encrypt_f64(20.0, &pk_b, &pk_a, &ctx, &mut rng);
    let ct2 = rns_encrypt_f64(8.0, &pk_b, &pk_a, &ctx, &mut rng);

    let ct_diff = rns_ct_sub(&ct1, &ct2);
    let dec = rns_decrypt_f64(&ct_diff, &s, &ctx);

    assert!(
        (dec - 12.0).abs() < 1.0,
        "20 - 8 should be ~12, got {}",
        dec
    );
}

// ═══════════════════════════════════════════════════════════════════════════
// SECTION 9: Cross-cutting attack — scalar_mul as NaN scale laundering
//            (Regression test — documents the full attack chain)
// ═══════════════════════════════════════════════════════════════════════════

/// Pre-R9 attack chain: Start with a NaN-scale ciphertext, use scalar_mul
/// (which had no scale check) to "launder" it past downstream operations.
///
/// Chain: corrupt scale -> scalar_mul -> add (would also NaN) -> propagate
///
/// After R9, scalar_mul catches the NaN at the first step.
#[test]
#[should_panic(expected = "rns_ct_scalar_mul: ct.scale must be finite and positive")]
fn r9_attack_chain_scalar_mul_laundering() {
    let mut rng = test_rng_seed(9080);
    let ctx = RnsCkksContext::new(3);
    let (_s, pk_b, pk_a) = rns_keygen(&ctx, &mut rng);

    let ct_good = rns_encrypt_f64(1.0, &pk_b, &pk_a, &ctx, &mut rng);
    let mut ct_bad = ct_good.clone();
    ct_bad.scale = f64::NAN;

    // Step 1: scalar_mul was the "laundering" step (no scale check pre-R9)
    let ct_laundered = rns_ct_scalar_mul(&ct_bad, 1); // R9 catches this!

    // Steps 2+3 would have succeeded pre-R9 and propagated NaN everywhere
    // let _ = rns_ct_add(&ct_laundered, &ct_good);
}

/// Pre-R9 attack chain: Use add_scalar_broadcast with corrupted ct.scale
/// to inject garbage into a valid computation.
#[test]
#[should_panic(expected = "rns_ct_add_scalar_broadcast: ct.scale must be finite and positive")]
fn r9_attack_chain_broadcast_injection() {
    let mut rng = test_rng_seed(9081);
    let ctx = RnsCkksContext::new(3);
    let (_s, pk_b, pk_a) = rns_keygen(&ctx, &mut rng);

    let mut ct = rns_encrypt_f64(1.0, &pk_b, &pk_a, &ctx, &mut rng);
    ct.scale = f64::NAN;

    // Pre-R9: encode_simd with NaN scale would produce garbage polynomial
    // that gets added to the ciphertext, corrupting it silently
    let _ = rns_ct_add_scalar_broadcast(&ct, 0.0);
}

// ═══════════════════════════════════════════════════════════════════════════
// SECTION 10: Auth tag coverage after R9 ops (integration)
// ═══════════════════════════════════════════════════════════════════════════

/// Verify that auth tags correctly detect tampering through scalar_mul path.
#[test]
fn r9_auth_tag_scalar_mul_integrity() {
    let mut rng = test_rng_seed(9090);
    let ctx = RnsCkksContext::new(3);
    let (_s, pk_b, pk_a) = rns_keygen(&ctx, &mut rng);
    let mac_key = [0xCDu8; 32];

    let mut ct = rns_encrypt_f64(5.0, &pk_b, &pk_a, &ctx, &mut rng);
    ct.authenticate(&mac_key);
    assert!(ct.verify_auth(&mac_key));

    // Scalar mul strips auth_tag (result has None)
    let ct_scaled = rns_ct_scalar_mul(&ct, 3);
    assert!(
        ct_scaled.auth_tag.is_none(),
        "scalar_mul should strip auth_tag"
    );

    // Re-authenticate after operation
    let mut ct_reauthed = ct_scaled;
    ct_reauthed.authenticate(&mac_key);
    assert!(ct_reauthed.verify_auth(&mac_key));

    // Tampering after re-auth should fail
    let mut tampered = ct_reauthed.clone();
    tampered.scale *= 2.0;
    assert!(
        !tampered.verify_auth(&mac_key),
        "tampered scale should fail auth"
    );
}

/// Verify that rotation also strips auth_tag.
#[test]
fn r9_auth_tag_rotation_strips_tag() {
    let mut rng = test_rng_seed(9091);
    let ctx = RnsCkksContext::new(3);
    let (s, pk_b, pk_a) = rns_keygen(&ctx, &mut rng);
    let rot_keys = rns_gen_rotation_keys(&s, &[1], &ctx, &mut rng);
    let mac_key = [0xEFu8; 32];

    let x_rep = replicate_vector(&[1.0, 2.0, 3.0, 4.0], 4);
    let mut ct = rns_encrypt_simd(&x_rep, &pk_b, &pk_a, &ctx, &mut rng);
    ct.authenticate(&mac_key);
    assert!(ct.verify_auth(&mac_key));

    let ct_rot = rns_rotate(&ct, 1, &rot_keys, &ctx);
    assert!(
        ct_rot.auth_tag.is_none(),
        "rotation should strip auth_tag (needs re-authentication)"
    );
}
