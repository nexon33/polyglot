//! Round 8 Penetration Test — Attack Tests for poly-client
//!
//! Focus areas:
//! - CKKS ct-ct multiply NaN/Inf scale bypass (rns_ct_mul, rns_ct_mul_leveled)
//! - Plaintext-ct multiply all-zero destruction
//! - Eval key digit count mismatch in relinearization
//! - Mod-switch NaN/Inf scale propagation
//! - Compression decompression bomb (streaming size limit)
//! - Compression header manipulation (v2 original_size=0, element_size tamper)
//! - Rotation edge cases (NUM_SLOTS wrap, missing key)
//! - SIMD encoding with subnormal/extreme float values
//! - Protocol InferRequest field edge cases
//! - MockEncryption deterministic key observation

#![cfg(feature = "ckks")]
#![allow(unused_variables)]

use poly_client::ckks::compress::{self, CompressionLevel};
use poly_client::ckks::rns::RnsPoly;
use poly_client::ckks::rns_ckks::*;
use poly_client::ckks::simd;
// rns_fhe_layer used in section 13 (NaN weight matrix test)

use rand::rngs::StdRng;
use rand::SeedableRng;

fn test_rng_seed(seed: u64) -> StdRng {
    StdRng::seed_from_u64(seed)
}

// ═══════════════════════════════════════════════════════════════════════════
// SECTION 1: rns_ct_mul NaN/Inf scale bypass (HIGH — R8 fix)
// ═══════════════════════════════════════════════════════════════════════════

/// R8-VULN-01: rns_ct_mul with NaN scale should panic.
/// Prior to R8, NaN scales passed through assert_eq (NaN != NaN is true, but
/// assert_eq uses PartialEq which returns false for NaN, so the assertion
/// actually fires on mismatch — but if BOTH are NaN, the scale check passes
/// because assert_eq compares with ==, and NaN == NaN is false, which means
/// assert_eq *would* panic. However the product scale NaN*NaN = NaN propagates.
/// The R8 fix adds explicit is_finite() + positive checks before the assert_eq.
#[test]
#[should_panic(expected = "rns_ct_mul: a.scale must be finite and positive")]
fn r8_rns_ct_mul_nan_scale_a() {
    let mut rng = test_rng_seed(801);
    let ctx = RnsCkksContext::new(3);
    let (s, pk_b, pk_a) = rns_keygen(&ctx, &mut rng);

    let ct = rns_encrypt_f64(1.0, &pk_b, &pk_a, &ctx, &mut rng);
    let mut ct_nan = ct.clone();
    ct_nan.scale = f64::NAN;

    // Should panic with R8 fix
    let _ = rns_ct_mul(&ct_nan, &ct, &ctx);
}

#[test]
#[should_panic(expected = "rns_ct_mul: b.scale must be finite and positive")]
fn r8_rns_ct_mul_nan_scale_b() {
    let mut rng = test_rng_seed(802);
    let ctx = RnsCkksContext::new(3);
    let (s, pk_b, pk_a) = rns_keygen(&ctx, &mut rng);

    let ct = rns_encrypt_f64(1.0, &pk_b, &pk_a, &ctx, &mut rng);
    let mut ct_nan = ct.clone();
    ct_nan.scale = f64::NAN;

    let _ = rns_ct_mul(&ct, &ct_nan, &ctx);
}

#[test]
#[should_panic(expected = "rns_ct_mul: a.scale must be finite and positive")]
fn r8_rns_ct_mul_inf_scale() {
    let mut rng = test_rng_seed(803);
    let ctx = RnsCkksContext::new(3);
    let (s, pk_b, pk_a) = rns_keygen(&ctx, &mut rng);

    let ct = rns_encrypt_f64(1.0, &pk_b, &pk_a, &ctx, &mut rng);
    let mut ct_inf = ct.clone();
    ct_inf.scale = f64::INFINITY;

    let _ = rns_ct_mul(&ct_inf, &ct, &ctx);
}

#[test]
#[should_panic(expected = "rns_ct_mul: a.scale must be finite and positive")]
fn r8_rns_ct_mul_neg_inf_scale() {
    let mut rng = test_rng_seed(804);
    let ctx = RnsCkksContext::new(3);
    let (s, pk_b, pk_a) = rns_keygen(&ctx, &mut rng);

    let ct = rns_encrypt_f64(1.0, &pk_b, &pk_a, &ctx, &mut rng);
    let mut ct_neg_inf = ct.clone();
    ct_neg_inf.scale = f64::NEG_INFINITY;

    let _ = rns_ct_mul(&ct_neg_inf, &ct, &ctx);
}

#[test]
#[should_panic(expected = "rns_ct_mul: a.scale must be finite and positive")]
fn r8_rns_ct_mul_zero_scale() {
    let mut rng = test_rng_seed(805);
    let ctx = RnsCkksContext::new(3);
    let (s, pk_b, pk_a) = rns_keygen(&ctx, &mut rng);

    let ct = rns_encrypt_f64(1.0, &pk_b, &pk_a, &ctx, &mut rng);
    let mut ct_zero = ct.clone();
    ct_zero.scale = 0.0;

    let _ = rns_ct_mul(&ct_zero, &ct, &ctx);
}

#[test]
#[should_panic(expected = "rns_ct_mul: a.scale must be finite and positive")]
fn r8_rns_ct_mul_negative_scale() {
    let mut rng = test_rng_seed(806);
    let ctx = RnsCkksContext::new(3);
    let (s, pk_b, pk_a) = rns_keygen(&ctx, &mut rng);

    let ct = rns_encrypt_f64(1.0, &pk_b, &pk_a, &ctx, &mut rng);
    let mut ct_neg = ct.clone();
    ct_neg.scale = -1.0;

    let _ = rns_ct_mul(&ct_neg, &ct, &ctx);
}

// ═══════════════════════════════════════════════════════════════════════════
// SECTION 2: rns_ct_mul_leveled NaN/Inf scale bypass (HIGH — R8 fix)
// ═══════════════════════════════════════════════════════════════════════════

#[test]
#[should_panic(expected = "rns_ct_mul_leveled: a.scale must be finite and positive")]
fn r8_rns_ct_mul_leveled_nan_scale_a() {
    let mut rng = test_rng_seed(811);
    let ctx = RnsCkksContext::new(3);
    let (s, pk_b, pk_a) = rns_keygen(&ctx, &mut rng);

    let ct = rns_encrypt_f64(1.0, &pk_b, &pk_a, &ctx, &mut rng);
    let mut ct_nan = ct.clone();
    ct_nan.scale = f64::NAN;

    let _ = rns_ct_mul_leveled(&ct_nan, &ct, &ctx);
}

#[test]
#[should_panic(expected = "rns_ct_mul_leveled: b.scale must be finite and positive")]
fn r8_rns_ct_mul_leveled_nan_scale_b() {
    let mut rng = test_rng_seed(812);
    let ctx = RnsCkksContext::new(3);
    let (s, pk_b, pk_a) = rns_keygen(&ctx, &mut rng);

    let ct = rns_encrypt_f64(1.0, &pk_b, &pk_a, &ctx, &mut rng);
    let mut ct_nan = ct.clone();
    ct_nan.scale = f64::NAN;

    let _ = rns_ct_mul_leveled(&ct, &ct_nan, &ctx);
}

#[test]
#[should_panic(expected = "rns_ct_mul_leveled: a.scale must be finite and positive")]
fn r8_rns_ct_mul_leveled_inf_scale() {
    let mut rng = test_rng_seed(813);
    let ctx = RnsCkksContext::new(3);
    let (s, pk_b, pk_a) = rns_keygen(&ctx, &mut rng);

    let ct = rns_encrypt_f64(1.0, &pk_b, &pk_a, &ctx, &mut rng);
    let mut ct_inf = ct.clone();
    ct_inf.scale = f64::INFINITY;

    let _ = rns_ct_mul_leveled(&ct_inf, &ct, &ctx);
}

#[test]
#[should_panic(expected = "rns_ct_mul_leveled: a.scale must be finite and positive")]
fn r8_rns_ct_mul_leveled_zero_scale() {
    let mut rng = test_rng_seed(814);
    let ctx = RnsCkksContext::new(3);
    let (s, pk_b, pk_a) = rns_keygen(&ctx, &mut rng);

    let ct = rns_encrypt_f64(1.0, &pk_b, &pk_a, &ctx, &mut rng);
    let mut ct_zero = ct.clone();
    ct_zero.scale = 0.0;

    let _ = rns_ct_mul_leveled(&ct_zero, &ct, &ctx);
}

// ═══════════════════════════════════════════════════════════════════════════
// SECTION 3: rns_ct_mul_plain_simd all-zero plaintext (HIGH — R8 fix)
// ═══════════════════════════════════════════════════════════════════════════

/// R8-VULN-03: Multiplying ciphertext by all-zero plaintext destroys content.
/// This is the plaintext-vector analogue of the scalar=0 attack (R6).
/// An attacker who controls the plaintext vector can silently zero out
/// the encrypted message without detection.
///
/// Note: The fix is applied at the rns_matvec level (skip zero diagonals)
/// rather than in rns_ct_mul_plain_simd, because zero-vector multiply is
/// a legitimate operation that occurs naturally for sparse matrices.
/// This test documents the behavior.
#[test]
fn r8_rns_ct_mul_plain_simd_all_zeros_produces_zero() {
    let mut rng = test_rng_seed(821);
    let ctx = RnsCkksContext::new(3);
    let (s, pk_b, pk_a) = rns_keygen(&ctx, &mut rng);

    let ct = rns_encrypt_simd(&[1.0, 2.0, 3.0], &pk_b, &pk_a, &ctx, &mut rng);
    let zeros = vec![0.0; 3];

    // Multiplying by all-zeros produces effectively zero (with noise)
    let ct_prod = rns_ct_mul_plain_simd(&ct, &zeros, &ctx);
    let ct_rescaled = rns_rescale(&ct_prod);
    let dec = rns_decrypt_simd(&ct_rescaled, &s, &ctx, 3);

    // All slots should decode to approximately zero (message destroyed)
    for i in 0..3 {
        assert!(
            dec[i].abs() < 1.0,
            "slot {} after zero-multiply: expected ~0, got {} (message destroyed as expected)",
            i, dec[i]
        );
    }
}

/// R8: rns_matvec now skips all-zero diagonals, which fixes the performance
/// and noise issue for sparse matrices (like identity matrices).
#[test]
fn r8_matvec_identity_skips_zero_diagonals() {
    let mut rng = test_rng_seed(822);
    let ctx = RnsCkksContext::new(3);
    let (s, pk_b, pk_a) = rns_keygen(&ctx, &mut rng);

    let d = 4;
    let mut identity = vec![0.0f64; d * d];
    for i in 0..d {
        identity[i * d + i] = 1.0;
    }

    let x = vec![1.0, 2.0, 3.0, 4.0];
    let x_rep = replicate_vector(&x, d);
    let rotations: Vec<i32> = (1..d as i32).collect();
    let rot_keys = rns_gen_rotation_keys(&s, &rotations, &ctx, &mut rng);

    let ct_x = rns_encrypt_simd(&x_rep, &pk_b, &pk_a, &ctx, &mut rng);
    let ct_result = rns_matvec(&ct_x, &identity, d, &rot_keys, &ctx);
    let ct_result = rns_rescale(&ct_result);

    let decrypted = rns_decrypt_simd(&ct_result, &s, &ctx, d);

    // I * x = x (with some noise)
    for i in 0..d {
        assert!(
            (decrypted[i] - x[i]).abs() < 1.0,
            "slot {} identity matvec: expected {}, got {}",
            i, x[i], decrypted[i]
        );
    }
}

/// Verify that a plaintext with one nonzero is correctly handled.
#[test]
fn r8_rns_ct_mul_plain_simd_one_nonzero_accepted() {
    let mut rng = test_rng_seed(823);
    let ctx = RnsCkksContext::new(3);
    let (s, pk_b, pk_a) = rns_keygen(&ctx, &mut rng);

    let ct = rns_encrypt_simd(&[5.0, 10.0], &pk_b, &pk_a, &ctx, &mut rng);
    let mut values = vec![0.0; simd::NUM_SLOTS];
    values[0] = 2.0; // One nonzero

    let ct_prod = rns_ct_mul_plain_simd(&ct, &values, &ctx);
    let ct_rescaled = rns_rescale(&ct_prod);
    let dec = rns_decrypt_simd(&ct_rescaled, &s, &ctx, 2);

    // Slot 0 should be 5.0 * 2.0 = 10.0
    assert!(
        (dec[0] - 10.0).abs() < 1.0,
        "slot 0 expected ~10.0, got {}",
        dec[0]
    );
}

// ═══════════════════════════════════════════════════════════════════════════
// SECTION 4: rns_relinearize eval key digit mismatch (HIGH — R8 fix)
// ═══════════════════════════════════════════════════════════════════════════

/// R8-VULN-04: Eval key generated for fewer primes than the ciphertext.
/// Without the R8 check, this would cause an index-out-of-bounds panic
/// deep inside the relinearization loop.
#[test]
#[should_panic(expected = "eval key has")]
fn r8_relin_eval_key_too_few_digits() {
    let mut rng = test_rng_seed(831);

    // Generate eval key for 3 primes
    let ctx3 = RnsCkksContext::new(3);
    let (s3, pk_b3, pk_a3) = rns_keygen(&ctx3, &mut rng);
    let evk_small = rns_gen_eval_key(&s3, &ctx3, &mut rng);

    // Generate ciphertext at 10 primes (needs more digits)
    let ctx10 = RnsCkksContext::new(10);
    let s10 = RnsPoly::from_coeffs(&s3.to_coeffs(), 10);
    let (_, pk_b10, pk_a10) = rns_keygen(&ctx10, &mut rng);

    let ct1 = rns_encrypt_f64(3.0, &pk_b10, &pk_a10, &ctx10, &mut rng);
    let ct2 = rns_encrypt_f64(4.0, &pk_b10, &pk_a10, &ctx10, &mut rng);

    // Multiply produces a triple at 10 primes
    let triple = rns_ct_mul(&ct1, &ct2, &ctx10);

    // Relinearize with undersized eval key should panic
    let _ = rns_relinearize(triple, &evk_small, &ctx10);
}

// ═══════════════════════════════════════════════════════════════════════════
// SECTION 5: rns_ct_mod_switch_to NaN/Inf scale (HIGH — R8 fix)
// ═══════════════════════════════════════════════════════════════════════════

#[test]
#[should_panic(expected = "rns_ct_mod_switch_to: scale must be finite and positive")]
fn r8_mod_switch_nan_scale() {
    let mut rng = test_rng_seed(841);
    let ctx = RnsCkksContext::new(5);
    let (_s, pk_b, pk_a) = rns_keygen(&ctx, &mut rng);

    let mut ct = rns_encrypt_f64(1.0, &pk_b, &pk_a, &ctx, &mut rng);
    ct.scale = f64::NAN;

    let _ = rns_ct_mod_switch_to(&ct, 3);
}

#[test]
#[should_panic(expected = "rns_ct_mod_switch_to: scale must be finite and positive")]
fn r8_mod_switch_inf_scale() {
    let mut rng = test_rng_seed(842);
    let ctx = RnsCkksContext::new(5);
    let (_s, pk_b, pk_a) = rns_keygen(&ctx, &mut rng);

    let mut ct = rns_encrypt_f64(1.0, &pk_b, &pk_a, &ctx, &mut rng);
    ct.scale = f64::INFINITY;

    let _ = rns_ct_mod_switch_to(&ct, 3);
}

#[test]
#[should_panic(expected = "rns_ct_mod_switch_to: scale must be finite and positive")]
fn r8_mod_switch_negative_scale() {
    let mut rng = test_rng_seed(843);
    let ctx = RnsCkksContext::new(5);
    let (_s, pk_b, pk_a) = rns_keygen(&ctx, &mut rng);

    let mut ct = rns_encrypt_f64(1.0, &pk_b, &pk_a, &ctx, &mut rng);
    ct.scale = -42.0;

    let _ = rns_ct_mod_switch_to(&ct, 3);
}

// ═══════════════════════════════════════════════════════════════════════════
// SECTION 6: Compression decompression bomb (HIGH — R8 fix)
// ═══════════════════════════════════════════════════════════════════════════

/// R8-VULN-06: Crafted v1 header with fraudulent original_size but real
/// zstd payload. The R8 streaming limiter ensures we never allocate more
/// than MAX_DECOMPRESSED_SIZE bytes.
#[test]
fn r8_compress_v1_fraudulent_original_size() {
    // Compress a small payload
    let data = vec![42u32; 100];
    let compressed = compress::compress(&data).unwrap();

    // Tamper: set original_size to MAX+1 in the header
    let mut tampered = compressed.clone();
    let fake_size: u32 = (33 * 1024 * 1024) as u32; // 33MB > 32MB limit
    tampered[5..9].copy_from_slice(&fake_size.to_le_bytes());

    let result: Result<Vec<u32>, _> = compress::decompress(&tampered);
    assert!(result.is_err(), "Should reject fraudulent original_size exceeding limit");
}

/// R8-VULN-06b: Crafted v2 header with zero original_size.
#[test]
fn r8_compress_v2_zero_original_size() {
    // Compress a real payload to get a valid v2 container
    let data = vec![42i64; 100];
    let compressed = compress::compress_with(&data, CompressionLevel::Compact).unwrap();

    // Tamper: set original_size to 0
    let mut tampered = compressed.clone();
    tampered[8..12].copy_from_slice(&0u32.to_le_bytes());

    let result: Result<Vec<i64>, _> = compress::decompress(&tampered);
    assert!(result.is_err(), "Should reject v2 with original_size=0 (size mismatch)");
}

/// Verify legitimate compression still works after R8 streaming limiter.
#[test]
fn r8_compress_legitimate_roundtrip_still_works() {
    let data: Vec<i64> = (0..8192).collect();

    // V1 (Lossless)
    let compressed_v1 = compress::compress(&data).unwrap();
    let decompressed_v1: Vec<i64> = compress::decompress(&compressed_v1).unwrap();
    assert_eq!(data, decompressed_v1);

    // V2 (Compact)
    let compressed_v2 = compress::compress_with(&data, CompressionLevel::Compact).unwrap();
    let decompressed_v2: Vec<i64> = compress::decompress(&compressed_v2).unwrap();
    assert_eq!(data, decompressed_v2);

    // V2 (Max)
    let compressed_max = compress::compress_with(&data, CompressionLevel::Max).unwrap();
    let decompressed_max: Vec<i64> = compress::decompress(&compressed_max).unwrap();
    assert_eq!(data, decompressed_max);
}

/// R8: Verify the bad magic path is still rejected.
#[test]
fn r8_compress_bad_magic_rejected() {
    let result: Result<Vec<u32>, _> = compress::decompress(b"BADMagicxxxxxxxxxxx");
    assert!(result.is_err());
}

/// R8: Verify bad version number is rejected.
#[test]
fn r8_compress_bad_version_rejected() {
    let mut data = vec![0u8; 20];
    data[..4].copy_from_slice(b"PFHE");
    data[4] = 255; // invalid version
    let result: Result<Vec<u32>, _> = compress::decompress(&data);
    assert!(result.is_err());
}

/// R8: Verify v2 with wrong element_size is rejected.
#[test]
fn r8_compress_v2_wrong_element_size() {
    let data = vec![42i64; 100];
    let compressed = compress::compress_with(&data, CompressionLevel::Compact).unwrap();

    // Tamper: change element_size from 8 to 4
    let mut tampered = compressed.clone();
    tampered[7] = 4; // wrong element size

    let result: Result<Vec<i64>, _> = compress::decompress(&tampered);
    assert!(result.is_err(), "Should reject v2 with mismatched element_size");
}

// ═══════════════════════════════════════════════════════════════════════════
// SECTION 7: Rotation edge cases (MEDIUM — tests only)
// ═══════════════════════════════════════════════════════════════════════════

/// Rotation by NUM_SLOTS (N/2 = 2048) wraps to identity.
/// The normalization in rotation_to_galois computes r % slots = 0 → identity.
/// This is correct behavior but could be surprising to callers.
#[test]
fn r8_rotation_by_num_slots_is_identity() {
    let galois = rotation_to_galois(simd::NUM_SLOTS as i32);
    assert_eq!(galois, 1, "rotation by NUM_SLOTS should map to identity (galois=1)");

    // Also: 2*NUM_SLOTS, -NUM_SLOTS
    assert_eq!(rotation_to_galois(2 * simd::NUM_SLOTS as i32), 1);
    assert_eq!(rotation_to_galois(-(simd::NUM_SLOTS as i32)), 1);
}

/// Rotation by negative amounts should work correctly.
#[test]
fn r8_rotation_negative_galois() {
    // rotation -1 should give the inverse of rotation +1
    let g_pos1 = rotation_to_galois(1);
    let g_neg1 = rotation_to_galois(-1);

    // g^1 * g^(-1) should equal 1 mod 2N (identity composition)
    let two_n = 2 * 4096;
    let product = (g_pos1 * g_neg1) % two_n;
    // The product should be 1 (identity) since rotating +1 then -1 cancels
    assert_eq!(product, 1, "g^1 * g^(-1) mod 2N should be 1 (identity)");
}

/// Rotation without a key should panic.
#[test]
#[should_panic(expected = "no rotation key for rotation")]
fn r8_rotation_missing_key_panics() {
    let mut rng = test_rng_seed(871);
    let ctx = RnsCkksContext::new(3);
    let (s, pk_b, pk_a) = rns_keygen(&ctx, &mut rng);

    // Generate key for rotation 1 only
    let rot_keys = rns_gen_rotation_keys(&s, &[1], &ctx, &mut rng);

    let ct = rns_encrypt_simd(&[1.0, 2.0, 3.0], &pk_b, &pk_a, &ctx, &mut rng);

    // Try rotation by 5 — no key exists
    let _ = rns_rotate(&ct, 5, &rot_keys, &ctx);
}

// ═══════════════════════════════════════════════════════════════════════════
// SECTION 8: SIMD encoding with extreme values (MEDIUM — tests only)
// ═══════════════════════════════════════════════════════════════════════════

/// Subnormal float values lose precision after SIMD encode/decode.
/// The scaling by delta and rounding to i64 obliterates subnormal precision.
#[test]
fn r8_simd_encode_subnormal_values() {
    let delta = (1u64 << 36) as f64;
    let subnormal = f64::MIN_POSITIVE / 2.0; // subnormal

    let values = vec![subnormal; 4];
    let coeffs = simd::encode_simd(&values, delta);
    let decoded = simd::decode_simd(&coeffs, delta, 4);

    // Subnormal * delta ≈ 0 after rounding to i64, so decoded ≈ 0
    for i in 0..4 {
        assert!(
            decoded[i].abs() < 1e-10,
            "slot {} subnormal: expected ~0, got {} (precision lost as expected)",
            i, decoded[i]
        );
    }
}

/// Very large float values may overflow i64 during encoding.
#[test]
fn r8_simd_encode_large_values() {
    let delta = (1u64 << 36) as f64;
    // Large value that overflows when scaled by delta:
    // 1e15 * 2^36 = 6.87e25 which exceeds i64::MAX ≈ 9.2e18
    let large = 1e15;

    let values = vec![large];
    let coeffs = simd::encode_simd(&values, delta);
    let decoded = simd::decode_simd(&coeffs, delta, 1);

    // The result wraps around modularly — large values produce garbage
    // This documents the behavior (no crash, but incorrect result)
    let error = (decoded[0] - large).abs() / large;
    // Don't assert error is small — it won't be. Just verify it doesn't crash.
    assert!(decoded[0].is_finite(), "decoded value should at least be finite");
    println!(
        "r8_simd_encode_large: input={}, decoded={}, rel_error={}",
        large, decoded[0], error
    );
}

/// Zero-length values should produce all-zero coefficients (padded).
#[test]
fn r8_simd_encode_empty_values() {
    let delta = (1u64 << 36) as f64;
    let values: Vec<f64> = vec![];
    let coeffs = simd::encode_simd(&values, delta);
    assert_eq!(coeffs.len(), 4096, "should produce N coefficients even for empty input");

    let decoded = simd::decode_simd(&coeffs, delta, 4);
    for i in 0..4 {
        assert!(
            decoded[i].abs() < 0.01,
            "slot {} empty input: expected ~0, got {}",
            i, decoded[i]
        );
    }
}

// ═══════════════════════════════════════════════════════════════════════════
// SECTION 9: rns_ct_mul_relin_leveled propagates scale fix (HIGH — R8 fix)
// ═══════════════════════════════════════════════════════════════════════════

/// The leveled multiply-relin wrapper should catch NaN/Inf via
/// rns_ct_mul_leveled's new R8 validation.
#[test]
#[should_panic(expected = "rns_ct_mul_leveled: a.scale must be finite and positive")]
fn r8_rns_ct_mul_relin_leveled_nan_propagates() {
    let mut rng = test_rng_seed(891);
    let ctx = RnsCkksContext::new(3);
    let (s, pk_b, pk_a) = rns_keygen(&ctx, &mut rng);
    let evk = rns_gen_eval_key(&s, &ctx, &mut rng);

    let ct = rns_encrypt_f64(2.0, &pk_b, &pk_a, &ctx, &mut rng);
    let mut ct_nan = ct.clone();
    ct_nan.scale = f64::NAN;

    // rns_ct_mul_relin_leveled calls rns_ct_mul_leveled which has R8 check
    let _ = rns_ct_mul_relin_leveled(&ct_nan, &ct, &evk, &ctx);
}

// ═══════════════════════════════════════════════════════════════════════════
// SECTION 10: Protocol InferRequest edge cases (MEDIUM — tests only)
// ═══════════════════════════════════════════════════════════════════════════

/// InferRequest with max_tokens=0 and temperature=0 should serialize fine.
/// There's no server-side validation of these fields at the protocol level.
#[test]
fn r8_infer_request_extreme_fields() {
    use poly_client::protocol::{InferRequest, Mode};

    let req_zero = InferRequest {
        model_id: "test".into(),
        mode: Mode::Encrypted,
        encrypted_input: vec![],
        max_tokens: 0,
        temperature: 0,
        seed: 0,
    };
    let json = serde_json::to_string(&req_zero).unwrap();
    let parsed: InferRequest = serde_json::from_str(&json).unwrap();
    assert_eq!(parsed.max_tokens, 0);
    assert_eq!(parsed.temperature, 0);

    // Max values
    let req_max = InferRequest {
        model_id: "test".into(),
        mode: Mode::Encrypted,
        encrypted_input: vec![],
        max_tokens: u32::MAX,
        temperature: u32::MAX,
        seed: u64::MAX,
    };
    let json2 = serde_json::to_string(&req_max).unwrap();
    let parsed2: InferRequest = serde_json::from_str(&json2).unwrap();
    assert_eq!(parsed2.max_tokens, u32::MAX);
}

/// InferRequest with empty model_id.
#[test]
fn r8_infer_request_empty_model_id() {
    use poly_client::protocol::{InferRequest, Mode};

    let req = InferRequest {
        model_id: String::new(),
        mode: Mode::Transparent,
        encrypted_input: vec![0u8; 1024],
        max_tokens: 50,
        temperature: 700,
        seed: 42,
    };
    let json = serde_json::to_string(&req).unwrap();
    let parsed: InferRequest = serde_json::from_str(&json).unwrap();
    assert!(parsed.model_id.is_empty());
}

// ═══════════════════════════════════════════════════════════════════════════
// SECTION 11: MockEncryption deterministic keys (MEDIUM — tests only)
// ═══════════════════════════════════════════════════════════════════════════

/// MockEncryption always returns the same hardcoded keys.
/// This is fine for development/testing but must never be used in production.
#[test]
fn r8_mock_encryption_deterministic_keys() {
    use poly_client::encryption::{EncryptionBackend, MockEncryption};

    let mock = MockEncryption;
    let (pk1, sk1) = mock.keygen();
    let (pk2, sk2) = mock.keygen();

    assert_eq!(pk1, pk2, "MockEncryption produces identical public keys");
    assert_eq!(sk1, sk2, "MockEncryption produces identical secret keys");
    assert_eq!(pk1, [0xAA; 32], "hardcoded public key bytes");
    assert_eq!(sk1, [0xBB; 32], "hardcoded secret key bytes");
}

/// MockEncryption ignores the key entirely.
#[test]
fn r8_mock_encryption_ignores_key() {
    use poly_client::encryption::{EncryptionBackend, MockEncryption};

    let mock = MockEncryption;
    let wrong_pk = [0xFF; 32];
    let wrong_sk = [0x00; 32];

    let data = vec![100, 200, 300];
    let ct = mock.encrypt(&data, &wrong_pk, &wrong_sk);
    let dec = mock.decrypt(&ct, &wrong_sk);

    assert_eq!(data, dec, "MockEncryption should work with any key");
}

// ═══════════════════════════════════════════════════════════════════════════
// SECTION 12: Auth tag integrity with R8 multiplication fixes
// ═══════════════════════════════════════════════════════════════════════════

/// Verify that the auth tag correctly protects against scale tampering
/// in the multiply path (the R8 scale validation is a first-line defense,
/// but auth tags are the second line).
#[test]
fn r8_auth_tag_detects_scale_tamper_for_mul_operand() {
    let mut rng = test_rng_seed(920);
    let ctx = RnsCkksContext::new(3);
    let (_s, pk_b, pk_a) = rns_keygen(&ctx, &mut rng);

    let mac_key = [0xABu8; 32];
    let mut ct = rns_encrypt_f64(5.0, &pk_b, &pk_a, &ctx, &mut rng);
    ct.authenticate(&mac_key);

    assert!(ct.verify_auth(&mac_key), "fresh auth should verify");

    // Tamper the scale
    let mut tampered = ct.clone();
    tampered.scale = ct.scale * 2.0;

    assert!(!tampered.verify_auth(&mac_key), "tampered scale should fail auth");
}

// ═══════════════════════════════════════════════════════════════════════════
// SECTION 13: FHE layer NaN/Inf in weight matrix (MEDIUM — tests only)
// ═══════════════════════════════════════════════════════════════════════════

/// NaN in weight matrix for rns_matvec should be caught by
/// rns_ct_mul_plain_simd's NaN check (R7).
#[test]
#[should_panic(expected = "all values must be finite")]
fn r8_fhe_layer_nan_weight_matrix() {
    let mut rng = test_rng_seed(930);
    let ctx = RnsCkksContext::new(3);
    let (s, pk_b, pk_a) = rns_keygen(&ctx, &mut rng);
    let rot_keys = rns_gen_rotation_keys(&s, &[1, 2, 3], &ctx, &mut rng);

    let ct = rns_encrypt_simd(&[1.0, 2.0, 3.0, 4.0], &pk_b, &pk_a, &ctx, &mut rng);

    // Weight matrix with NaN
    let d = 4;
    let mut weights = vec![1.0f64; d * d];
    weights[5] = f64::NAN; // inject NaN

    // rns_matvec calls rns_ct_mul_plain_simd which has the NaN check
    let _ = rns_matvec(&ct, &weights, d, &rot_keys, &ctx);
}

// ═══════════════════════════════════════════════════════════════════════════
// SECTION 14: Entropy check on all-zero ciphertext (MEDIUM — tests only)
// ═══════════════════════════════════════════════════════════════════════════

/// Verify that the entropy check flags a manually constructed "ciphertext"
/// with all-zero coefficients (would indicate implementation bug or attack).
#[test]
fn r8_entropy_check_synthetic_zero_ciphertext() {
    let zero_ct = RnsCiphertext {
        c0: RnsPoly::zero(3),
        c1: RnsPoly::zero(3),
        scale: (1u64 << 36) as f64,
        level: 0,
        auth_tag: None,
    };

    let check = compress::entropy_check(&zero_ct);
    assert!(
        !check.pass,
        "All-zero synthetic ciphertext should FAIL entropy check (ratio={:.2}x)",
        check.ratio
    );
}

/// Real ciphertexts should pass entropy check.
#[test]
fn r8_entropy_check_real_ciphertext_passes() {
    let mut rng = test_rng_seed(941);
    let ctx = RnsCkksContext::new(3);
    let (_s, pk_b, pk_a) = rns_keygen(&ctx, &mut rng);

    let ct = rns_encrypt_simd(
        &[1.0, 2.0, 3.0, 4.0, 5.0],
        &pk_b, &pk_a, &ctx, &mut rng
    );

    let check = compress::entropy_check(&ct);
    assert!(
        check.pass,
        "Real ciphertext should PASS entropy check (ratio={:.2}x, threshold={:.1}x)",
        check.ratio, check.threshold
    );
}

// ═══════════════════════════════════════════════════════════════════════════
// SECTION 15: Correct multiply operation after R8 fixes
// ═══════════════════════════════════════════════════════════════════════════

/// Verify that legitimate ct-ct multiply still works correctly after R8 fixes.
#[test]
fn r8_ct_mul_legitimate_still_works() {
    let mut rng = test_rng_seed(950);
    let ctx = RnsCkksContext::new(3);
    let (s, pk_b, pk_a) = rns_keygen(&ctx, &mut rng);
    let evk = rns_gen_eval_key(&s, &ctx, &mut rng);

    let ct1 = rns_encrypt_f64(3.0, &pk_b, &pk_a, &ctx, &mut rng);
    let ct2 = rns_encrypt_f64(4.0, &pk_b, &pk_a, &ctx, &mut rng);

    let ct_prod = rns_ct_mul_relin(&ct1, &ct2, &evk, &ctx);
    let ct_rescaled = rns_rescale(&ct_prod);
    let dec = rns_decrypt_f64(&ct_rescaled, &s, &ctx);

    assert!(
        (dec - 12.0).abs() < 1.0,
        "3 * 4 should be ~12, got {}",
        dec
    );
}

/// Verify that legitimate leveled multiply still works correctly.
#[test]
fn r8_ct_mul_leveled_legitimate_still_works() {
    let mut rng = test_rng_seed(951);
    let ctx = RnsCkksContext::new(5);
    let (s, pk_b, pk_a) = rns_keygen(&ctx, &mut rng);
    let evk = rns_gen_eval_key(&s, &ctx, &mut rng);

    let ct1 = rns_encrypt_f64(2.0, &pk_b, &pk_a, &ctx, &mut rng);
    let ct2 = rns_encrypt_f64(5.0, &pk_b, &pk_a, &ctx, &mut rng);

    // Mod-switch ct1 to 3 primes first (different level from ct2)
    let ct1_switched = rns_ct_mod_switch_to(&ct1, 3);

    // Leveled multiply should handle the level mismatch
    let ct_prod = rns_ct_mul_relin_leveled(&ct1_switched, &ct2, &evk, &ctx);
    let ct_rescaled = rns_rescale(&ct_prod);
    let dec = rns_decrypt_f64(&ct_rescaled, &s, &ctx);

    assert!(
        (dec - 10.0).abs() < 1.0,
        "2 * 5 should be ~10, got {}",
        dec
    );
}

/// Verify legitimate mod-switch still works after R8 validation.
#[test]
fn r8_mod_switch_legitimate_still_works() {
    let mut rng = test_rng_seed(952);
    let ctx = RnsCkksContext::new(5);
    let (s, pk_b, pk_a) = rns_keygen(&ctx, &mut rng);

    let values = vec![1.5, -2.5, 3.14, 0.0];
    let ct = rns_encrypt_simd(&values, &pk_b, &pk_a, &ctx, &mut rng);

    let ct_switched = rns_ct_mod_switch_to(&ct, 3);
    assert_eq!(ct_switched.c0.num_primes, 3);

    let dec = rns_decrypt_simd(&ct_switched, &s, &ctx, 4);
    for i in 0..4 {
        assert!(
            (dec[i] - values[i]).abs() < 0.01,
            "slot {} mod-switch: expected {}, got {}",
            i, values[i], dec[i]
        );
    }
}
